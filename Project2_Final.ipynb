{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the orginal Dataset, Then re run and and scaling and splitting in order to balancec and clean data\n",
    "\n",
    "## Step 1, 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "# Found Data Set\n",
    "# Generally This dataset is about Econmic Freedoms in the world\n",
    "df = pd.read_csv('efw_cc.csv')\n",
    "dataframe_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_copy.loc[dataframe_copy['ECONOMIC FREEDOM'] > 5, 'Econ_Freedom_label'] = '1' # Economic Freedom is above 5\n",
    "dataframe_copy.loc[dataframe_copy['ECONOMIC FREEDOM'] <= 5, 'Econ_Freedom_label'] = '0'# Economic Freedom is between 0 and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Orginal Shape of DataFrame (3726, 36) \n",
      "Shape of dataframe after dropping values that are NA after Categorizing predictor: (3003, 37)\n"
     ]
    }
   ],
   "source": [
    "# I dropped all the rows that are NA in the Predicting categories label (Econ_Freedom_label)\n",
    "dataframe_copy2 = dataframe_copy.dropna(subset = [\"Econ_Freedom_label\"], inplace=False)\n",
    "print(\"\\nOrginal Shape of DataFrame\", df.shape, \"\\nShape of dataframe after dropping values that are NA after Categorizing predictor:\", dataframe_copy2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>ISO_code</th>\n",
       "      <th>countries</th>\n",
       "      <th>ECONOMIC FREEDOM</th>\n",
       "      <th>rank</th>\n",
       "      <th>quartile</th>\n",
       "      <th>1a_government_consumption</th>\n",
       "      <th>1b_transfers</th>\n",
       "      <th>1c_gov_enterprises</th>\n",
       "      <th>1d_top_marg_tax_rate</th>\n",
       "      <th>...</th>\n",
       "      <th>3_sound_money</th>\n",
       "      <th>4a_tariffs</th>\n",
       "      <th>4b_regulatory_trade_barriers</th>\n",
       "      <th>4c_black_market</th>\n",
       "      <th>4d_control_movement_capital_ppl</th>\n",
       "      <th>4_trade</th>\n",
       "      <th>5a_credit_market_reg</th>\n",
       "      <th>5b_labor_market_reg</th>\n",
       "      <th>5c_business_reg</th>\n",
       "      <th>5_regulation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Albania</td>\n",
       "      <td>7.54</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.232353</td>\n",
       "      <td>7.509902</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.553657</td>\n",
       "      <td>8.963556</td>\n",
       "      <td>7.489905</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>6.406138</td>\n",
       "      <td>8.214900</td>\n",
       "      <td>7.098562</td>\n",
       "      <td>6.916278</td>\n",
       "      <td>6.705863</td>\n",
       "      <td>6.906901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>4.99</td>\n",
       "      <td>159.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>7.817129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>7.253894</td>\n",
       "      <td>6.872533</td>\n",
       "      <td>2.481294</td>\n",
       "      <td>5.56391</td>\n",
       "      <td>1.590362</td>\n",
       "      <td>4.127025</td>\n",
       "      <td>5.100509</td>\n",
       "      <td>5.029513</td>\n",
       "      <td>5.676956</td>\n",
       "      <td>5.268992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Angola</td>\n",
       "      <td>5.17</td>\n",
       "      <td>155.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>8.886739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>...</td>\n",
       "      <td>5.606605</td>\n",
       "      <td>6.989244</td>\n",
       "      <td>2.024949</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>2.044823</td>\n",
       "      <td>5.264754</td>\n",
       "      <td>7.064905</td>\n",
       "      <td>4.560325</td>\n",
       "      <td>4.930271</td>\n",
       "      <td>5.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>4.84</td>\n",
       "      <td>160.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.335294</td>\n",
       "      <td>6.048930</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.614336</td>\n",
       "      <td>6.421600</td>\n",
       "      <td>4.811105</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.697482</td>\n",
       "      <td>3.982547</td>\n",
       "      <td>5.419820</td>\n",
       "      <td>5.151405</td>\n",
       "      <td>5.535831</td>\n",
       "      <td>5.369019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>ARM</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>7.57</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.264706</td>\n",
       "      <td>7.748532</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.521940</td>\n",
       "      <td>8.547556</td>\n",
       "      <td>7.194410</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>6.830998</td>\n",
       "      <td>8.143241</td>\n",
       "      <td>9.102046</td>\n",
       "      <td>6.234630</td>\n",
       "      <td>6.797530</td>\n",
       "      <td>7.378069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>1970</td>\n",
       "      <td>VEN</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>7.18</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.602003</td>\n",
       "      <td>9.827430</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9.713677</td>\n",
       "      <td>8.066667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.688889</td>\n",
       "      <td>9.679680</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.312277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3722</th>\n",
       "      <td>1970</td>\n",
       "      <td>VNM</td>\n",
       "      <td>Vietnam</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723</th>\n",
       "      <td>1970</td>\n",
       "      <td>YEM</td>\n",
       "      <td>Yemen, Rep.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>1970</td>\n",
       "      <td>ZMB</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.448131</td>\n",
       "      <td>9.105430</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.133689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.327327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>1970</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.806922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.036667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3726 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      year ISO_code    countries  ECONOMIC FREEDOM   rank  quartile  \\\n",
       "0     2016      ALB      Albania              7.54   34.0       1.0   \n",
       "1     2016      DZA      Algeria              4.99  159.0       4.0   \n",
       "2     2016      AGO       Angola              5.17  155.0       4.0   \n",
       "3     2016      ARG    Argentina              4.84  160.0       4.0   \n",
       "4     2016      ARM      Armenia              7.57   29.0       1.0   \n",
       "...    ...      ...          ...               ...    ...       ...   \n",
       "3721  1970      VEN    Venezuela              7.18   10.0       1.0   \n",
       "3722  1970      VNM      Vietnam               NaN    NaN       NaN   \n",
       "3723  1970      YEM  Yemen, Rep.               NaN    NaN       NaN   \n",
       "3724  1970      ZMB       Zambia               NaN    NaN       NaN   \n",
       "3725  1970      ZWE     Zimbabwe               NaN    NaN       NaN   \n",
       "\n",
       "      1a_government_consumption  1b_transfers  1c_gov_enterprises  \\\n",
       "0                      8.232353      7.509902                 8.0   \n",
       "1                      2.150000      7.817129                 0.0   \n",
       "2                      7.600000      8.886739                 0.0   \n",
       "3                      5.335294      6.048930                 6.0   \n",
       "4                      7.264706      7.748532                 8.0   \n",
       "...                         ...           ...                 ...   \n",
       "3721                   6.602003      9.827430                 7.0   \n",
       "3722                        NaN           NaN                 NaN   \n",
       "3723                        NaN           NaN                 NaN   \n",
       "3724                   3.448131      9.105430                 0.0   \n",
       "3725                   7.806922           NaN                 2.0   \n",
       "\n",
       "      1d_top_marg_tax_rate  ...  3_sound_money  4a_tariffs  \\\n",
       "0                      8.0  ...       9.553657    8.963556   \n",
       "1                      4.5  ...       7.253894    6.872533   \n",
       "2                      9.5  ...       5.606605    6.989244   \n",
       "3                      4.0  ...       5.614336    6.421600   \n",
       "4                      5.0  ...       9.521940    8.547556   \n",
       "...                    ...  ...            ...         ...   \n",
       "3721                   NaN  ...       9.713677    8.066667   \n",
       "3722                   NaN  ...            NaN         NaN   \n",
       "3723                   NaN  ...            NaN         NaN   \n",
       "3724                   NaN  ...       5.133689         NaN   \n",
       "3725                   NaN  ...       5.036667         NaN   \n",
       "\n",
       "      4b_regulatory_trade_barriers  4c_black_market  \\\n",
       "0                         7.489905         10.00000   \n",
       "1                         2.481294          5.56391   \n",
       "2                         2.024949         10.00000   \n",
       "3                         4.811105          0.00000   \n",
       "4                         7.194410         10.00000   \n",
       "...                            ...              ...   \n",
       "3721                           NaN         10.00000   \n",
       "3722                           NaN              NaN   \n",
       "3723                           NaN              NaN   \n",
       "3724                           NaN          0.00000   \n",
       "3725                           NaN          7.20000   \n",
       "\n",
       "      4d_control_movement_capital_ppl   4_trade  5a_credit_market_reg  \\\n",
       "0                            6.406138  8.214900              7.098562   \n",
       "1                            1.590362  4.127025              5.100509   \n",
       "2                            2.044823  5.264754              7.064905   \n",
       "3                            4.697482  3.982547              5.419820   \n",
       "4                            6.830998  8.143241              9.102046   \n",
       "...                               ...       ...                   ...   \n",
       "3721                         8.000000  8.688889              9.679680   \n",
       "3722                              NaN       NaN                   NaN   \n",
       "3723                              NaN       NaN                   NaN   \n",
       "3724                         2.000000       NaN              7.327327   \n",
       "3725                              NaN       NaN                   NaN   \n",
       "\n",
       "      5b_labor_market_reg  5c_business_reg  5_regulation  \n",
       "0                6.916278         6.705863      6.906901  \n",
       "1                5.029513         5.676956      5.268992  \n",
       "2                4.560325         4.930271      5.518500  \n",
       "3                5.151405         5.535831      5.369019  \n",
       "4                6.234630         6.797530      7.378069  \n",
       "...                   ...              ...           ...  \n",
       "3721                  NaN              NaN      5.312277  \n",
       "3722                  NaN              NaN           NaN  \n",
       "3723                  NaN              NaN           NaN  \n",
       "3724                  NaN              NaN           NaN  \n",
       "3725                  NaN              NaN           NaN  \n",
       "\n",
       "[3726 rows x 36 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataframe_copy2.drop(['ECONOMIC FREEDOM', 'Econ_Freedom_label', 'ISO_code', 'countries'], axis = 1)\n",
    "y = dataframe_copy2.Econ_Freedom_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.2,\n",
      "            train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "X_array = X.to_numpy()\n",
    "y_array = y.to_numpy()\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\n",
    "split.get_n_splits(X_array, y_array)\n",
    "print(split)\n",
    "for train_index , test_index in split.split(X_array, y_array):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train = X_array[train_index]\n",
    "    X_test = X_array[test_index]\n",
    "    y_train = y_array[train_index]\n",
    "    y_test = y_array[test_index]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes\n",
    "I kept this line of code in as kind of a default or check to make sure retest everything. \n",
    "``` from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)\n",
    "print(\"\\nTrain Value Spread\\n\", y_train.value_counts(),\n",
    "\"\\nTest Value Spread\\n\", y_test.value_counts())```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes\n",
    "I had to conver the array back to a dataframe in order to fill in the data missing values with the mean of the column. Take note here we're using the X_train as the way in which we fill in for X_test as well, in order to not create bias since we're not suppose to actually know what is going on with the testing dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train, columns = X.columns)\n",
    "X_test = pd.DataFrame(X_test, columns = X.columns)\n",
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_correlation = pd.concat([dataframe_copy['ECONOMIC FREEDOM'],X_train], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = dataframe_correlation.corr()\n",
    "corr_matrix_specific = np.abs(corr_matrix['ECONOMIC FREEDOM'])\n",
    "top_correlations = corr_matrix_specific.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_correlations = top_correlations['4_trade':'4b_regulatory_trade_barriers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_index = chosen_correlations.index\n",
    "X_train_corr = X_train.loc[:,correlation_index]\n",
    "X_test_corr = X_test.loc[:,correlation_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes \n",
    "I use again standard scaler as I did in project 1, in order to reduce the mean, I do this for two fold to cover potenital issues later on using SVM or Descision Trees, as well as it standardizes the data pretty well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler() \n",
    "scaler.fit(X_train_corr)\n",
    "X_train_scaled = pd.DataFrame(scaler.transform(X_train_corr),columns = X_train_corr.columns)  \n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_corr),columns = X_test_corr.columns)\n",
    "\n",
    "#print(\"\\nShape of Training set:\", X_train_scaled.shape, '\\nShape of Testing Set:', X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_scaled.copy()\n",
    "X_test = X_test_scaled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes \n",
    "Double checking to make sure that the Null values are gone and that I am not going forward with these results with missing values still. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4_trade                               0\n",
       "3_sound_money                         0\n",
       "4d_control_movement_capital_ppl       0\n",
       "2c_protection_property_rights         0\n",
       "2a_judicial_independence              0\n",
       "3b_std_inflation                      0\n",
       "4a_tariffs                            0\n",
       "2j_gender_adjustment                  0\n",
       "1d_top_marg_tax_rate                  0\n",
       "2h_reliability_police                 0\n",
       "5_regulation                          0\n",
       "rank                                  0\n",
       "2g_restrictions_sale_real_property    0\n",
       "4c_black_market                       0\n",
       "2f_legal_enforcement_contracts        0\n",
       "1_size_government                     0\n",
       "quartile                              0\n",
       "3a_money_growth                       0\n",
       "2b_impartial_courts                   0\n",
       "2_property_rights                     0\n",
       "3c_inflation                          0\n",
       "1a_government_consumption             0\n",
       "2i_business_costs_crime               0\n",
       "4b_regulatory_trade_barriers          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re Labeling the predictor column names correctly \n",
    "y_train = pd.Series(y_train)\n",
    "y_train = y_train.rename(\"Econ_Freedom_label\")\n",
    "\n",
    "y_test = pd.Series(y_test)\n",
    "y_test = y_test.rename(\"Econ_Freedom_label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWoAAAFqCAYAAAAz2BDjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABigUlEQVR4nO3dd3Qc5fXw8e9sl7Ra9WIV994bNsYFgwsY2/RuwLRAqAkklOQXILy04IQAwSmEEHAIoVcDBoMNGPfee5GtYvWyKtt33j/WXmm1K1ku0q6k+znH53hnZ2bv2NLV6JnnuVdRVVVFCCFExNKEOwAhhBDNk0QthBARThK1EEJEOEnUQggR4SRRCyFEhGs3idrtdpOXl4fb7Q53KEII0abaTaIuLCxkypQpFBYWhjsUIYRoU+0mUQshRGcliVoIISKcJGohhIhwkqiFECLCSaIWQogIJ4laCCEinCRqIYSIcLpwfOj8+fNZtGgRAOeeey4PP/xwOMIQQoh2oc3vqFeuXMny5cv55JNP+PTTT9mxYwfffvttW4chhBDtRpvfUaekpPDoo49iMBgA6NWrFwUFBQH7WK1WrFZrwDZZkSiE6KzaPFH36dPH//ecnBwWLVrEO++8E7DPggULmD9/fluH1mmVW+0s35JPjEnP+GEZmAxhGRETQjRBCVcrrn379nHnnXdy3333cdlllwW819Qd9Zw5c1iyZAlZWVltGWqHdrS0lgdf+pEamwuAXllxvHD/JLRaec4sRKQIy63Thg0buP/++/ntb3/LzJkzg963WCxYLJYwRNZ+FJfX8d26IyiKwvSxXUmKi2rxsUvXH+GTHw6gURSS403+JA1wIK+KjXuKOWtgemuELYQ4BW2eqI8ePco999zDiy++yLhx49r64zuEsiobv3zxB6rrfAn2qxWHmP/QecSZjSc8dndOOS++s8n/+mBBVdA+0u5YiMjS5on69ddfx+Fw8Ic//MG/7dprr+W6665r61DarR835vmTNEBljYPlm/OZOaHnCY/duKc4aJtep8Hl9gLQLT2WEf1Sz1ywQojT1uaJ+ne/+x2/+93v2vpjOxSjXhu0zRBiWyjduwQPKd171XDKrXZiTDrOHZmFXifj00JEEnm83w6dOyqbz386SEFpLQDZabFMGJ7ZomPHDenCzPE9+GZ1DqAwa0IPzh+d3XrBCiFOW9hmfZysvLw8pkyZIrM+jrE73KzeUYhGgbGDu2DUaymrsrHwp4NYa52cPzqbwb2Smzy+zu5CURSijPKzWohIJ9+l7YCqqmw/UEZVrYOR/VKJNukxGXVMHln/A6us0veAsbLGCcCSdUd49u4JDOqZ5N+noLSGHzfkEWXSM/WsbMzR+ja/FiHEyZNE3Q48t2Adq7YdBcASY+CZu86he5c4AA4VVPH1yhwWrz2M21P/y5FXhb9/tIVBPZPokmymV6aFx15dicf3zJAPluzl37+bhlEWtwgR8eS7NMLtPVLhT9IA1lonv/zzj9w6exAGvZa/frilyWMPF1ZzuLAaAK1W8Sfp4+f564dbefD6ka0WuxDizJBEHeFqGyxGOc7jVXn98+3ExhhafB6PJ/hRxLYDJacVmxCibcg8rAg3pHcymSnmoO1eFexOz2md23psPFsIEdkkUUc4nVbD8/dOYMygwCXd5ig9M8f3aPF5zFHBDw6dbi/lVjsANXVO/vjWeq5/bBG/+8cK8oqrTy9wIcQZI4m6HdBqFM4Zks7UMV3plh7L8D7JXDW1L6nxUdx9xVAmnmAOdWpCFOlJ0UHbo006/7Lzf366jWWb86muc7JlXynP/2d9q1yLEOLkyRh1hDtaWsuv/7IMa61vmGJE3xSMBi1vLNwBgMmg5WeXDuGnzflNnqO4wkZxhS1gm1ajkJls5unXV2M06Fi5NbAmeM5RK9ZaJ5aTGAcXQrQOSdQR7ovlB/1JGmDT3sAHgHanhy+XHyQzxUx+SU3Aew1reBxn0Gtwurx4vCr78iqb/eztB0o5Z2jG6V2AEOK0SaKOcA7XiR8YHizw1e4e0D2Rkf1TmTAsg0SLiR835fG3D7cG7Ot0eUOdIqSl63MlUQsRASRRR7jpY7vx3boj/ul12WlmYqMN7DxUHrTvrpxycouq2XmwjHFD0vnbR9tO67NbUjZVCNH6JFFHuGWb8v1JWqNAt3QLl5/Xm/15VXywZC8ljcaea2wuNu0tCRoiORUFpTXU2FwhZ4wIIdqOzPqIYL4iSwf8r70qLN9SwIMvLWPnwTLGDmrdLizbD5Tx4v82tOpnCCFOTO6oI1itzYW3idqGP2zMa5MY1u0qotbmIkbuqoUIG7mjjmBOd8sf/J0upYntqgp/eGtd0BCLEKLtSKKOYD+20V2zTqvQXFHyzXtKePrfa9okFiFEMEnUESzUYpMoY8tabp0Md4iCTY0dLKiiuLzujH+2EOLEJFFHsAvHdSc7Ldb/evSANP7x8BRmju/e5rHEROmJi5XpekKEgyTqCGaO0jOkdxJRRh0xJh2b9hQz96nFfLv2CAO6JwC+lYaDeyUxaURwvQ+9rqmR56YZ9MFfEgpw9xVDQzbVFUK0Ppn1EcHeWrSLr1bkBG13urzkHK3mvWcuQq/Totdp+HbtYZZtarreR0uFWrmowgkLPwkhWo/cUUew5hKvzeFGr9Og1/n+C3fnBK9UdLnPTN/i5DgTinLyd+dCiDND7qgjmMVsoKiJB3jRJh1lVb5a0guXH2Tjntbr1jJ7Uq9WO7cQ4sQkUUewGy7ozxOvrQ75Xp3dzR3PfofRoD3tTi8nkiAPEYUIKxn6iGA1IfolNqRy+u24WqJ7euyJdxJCtBpJ1BEsJSG4K0s4fLB0f7hDEKJTk0QdwSqO9TM8VZoz9L/70+Z8fjoDM0qEEKdGEnUEO905G94zWCpkw56iM3cyIcRJkUQdwUb1T0WriYxpcXZH64+FCyFCk0QdwUwGHcP6JIc7DMA3VVAIER6SqCNctCky6kCPG9wl3CEI0WnJPOoIt3ZneMeGFQVunTWIEf1SwxqHEJ2Z3FFHuLgwDzloFNDp5MtEiHCS78AIN35IRlg/3+OFVz/ZxoqtBWGNQ4jOTBJ1BKuzu/hixcFwhwHA+jAPwQjRmUmijmDLtxS0qPtKW+gqy8iFCBtJ1BHsu7WHwx0C4KvUN+Oc7uEOQ4hOSxJ1BMsvrg13CABkJMWg18qXihDhErbvvpqaGmbNmkVeXtt02m6PkuJN4Q4BgP35Vby/ZF+4wxCi0wpLot6yZQvXXXcdOTk54fj4dmNIr6Rwh+C3bmdhuEMQotMKy4KX999/nyeeeIKHH3445PtWqxWr1RqwrbCw8yWK9buKwx2CX8Nu6EKIthWWRP3MM880+/6CBQuYP39+G0UTuarrnOEOAQCdVuHGGQPCHYYQnVZELiGfO3cul112WcC2wsJC5syZE6aIwiM22kB1XfNdXtpCWmI0TpdUzxMiXCIyUVssFiwWS7jDCLthfVMoKA3/zI/8klr+sGAdf/n1eeEORYhOSeZcRbDeWXHhDsHv0FErdfbw390L0RlJoo5gXyw/FO4QAuzOqQh3CEJ0SmEd+li6dGk4Pz7ilVbYwh2CnwJkpZnDHYYQnZLcUUcwtzdyHuDdevEgUiOkK7oQnY0k6gim02jDHYJfl+SYcIcgRKcliTqCuc5kG/HTJGVOhQgfSdQRLDYqchrKfr8hD7vTHe4whOiUJFFHsMmjssIdgp/D5aEkgh5uCtGZSKKOYBeM7RbuEPwSLUYyU2TWhxDhIIk6gu3Prwp3CH4ulweNRgl3GEJ0SpKoI1hWBN3BVtvceL2R0RZMiM5GEnUES0uMRhdBnVV+2ixNHoQIh8jJAiLIvz7fjtsTOVP0EmIjo+OMEJ2NJOoIVlBSE+4QAgzoETkdZ4ToTCRRR7AJwzPDHUKA9btk0YsQ4SCJOoIlWSJrqKFv1/hwhyBEpySJOoL9tDk/3CH4zZrQg6S4qHCHIUSnJIk6ghVH0ErAOy8bGu4QhOi0JFFHsEiqWPf1qshqYiBEZyKJOoJdem4vtBGyGvDtr3ejqrLgRYhwkEQdwbqmW4iJ0oc7DAAqa5zkHLWGOwwhOiVJ1BEsp6AKa60z3GH4VddFTixCdCaSqCNYamI0kTHw4SMjH0KEhyTqCHaksJpIyo0mQ+S0BhOiM5FEHcH251aEOwQ/g16hZ2ZcuMMQolOSRB3BrHWucIfg1zXNgl4nd9RChIMk6ghWWlEX7hD8aiLoh4YQnY0k6ghWUFob7hD80pKiwx2CEJ2WJOoIlltUHe4Q/M4amB7uEITotCRRR7BIan3VJSlylrML0dlIoo5g8RFU5jTKqAt3CEJ0WpKoI1gk1aO2yqpEIcJGEnUEK6mIjIeJep2G/t0Swh2GEJ2WJOoIVufwhDsEAFxuL28t2hXuMESYeJ02qtZ+Qenif2PPla+DcJCBxwiWnhRDdV1luMMAYMPu4nCHIMKk8L1nsR/ZCYB13VekXf0oMX1GhzmqzkXuqCPY4J6R0/U7I4KaGIi24yw+4k/SPirWDd+ELZ7OShJ1BCuvdoQ7BL9zhmSEOwQRBoreELRNYzCGIZLOTRJ1BOuWHhvuEPzKrfZwhyDCQJ+QTuyw8/2vFYOJ+HGXhTGizknGqCOYRomcatTfb8jlltmDwh2GCIOUWfdgHnIu7spionqNRGeOD3dInY4k6gi2bFNeuEPwq7XJPOrOQvW4qVqzEFvONgzpPUgYfwVR3QZDt3BH1nlJoo5gkVSUKZLu7kXrKv/+v1StWQiA7dAWXKX5pF/9aJij6txkjDqCxcdGzkOb7tI0oNOo2bEi4HXdvvV4nfKMIpzCkqgXLlzIRRddxPTp03n77bfDEUK7cNvsweEOwW/yyKxwhyDaiM4SOC1UGxOH12GjdvcabIe3U7t7Ne6aypDHql4PdYe2YDu8HfvRA9TuXYfXacNdXUHl6s+xbvwWr8MWeIyqYjuyk7oDm1A97ta6rHatzYc+ioqKePHFF/n4448xGAxce+21jB07lt69e7d1KBEvyhQ5I1M5BdZwhyDaSNLUuRS+/we89hoUnYHY4VPJ/dvdqO765xSKVk/aFQ8R3WeUf5vXaaPgP4/hLDoUcD7FGI3qsMGxDqBl371J+rX/R832n3AcPYDXZsVdVQKAPjmLjJueQRtlbv0LbUfa/I565cqVnH322cTHxxMdHc0FF1zA119/HbCP1WolLy8v4E9hYWFbhxp2dmfk3F18vyFyHmyK1mXKHkDX+14l46an6Xr/P7Ed2hKQpAFUj4vyHwJ/G67Z9mNQkgZQHXXQoE2z6rJT9NGfqN60GGfhAX+SBnCV5lG9ZemZvaAOoM1v2YqLi0lJSfG/Tk1NZevWrQH7LFiwgPnz57d1aBHHEhM5Y9QerzfcIYg2pDGYMGUPAMBjC93AwlMXuN1VVdri83vrqpp+zya/vTXW5ona6/WiNJhBoKpqwGuAuXPnctllgZPqCwsLmTNnTpvEGCmqaiPnAY7bo+L2eNFp5flzZxM7bAoVPwQ/S4od7lsI47aWUvThH3Ec3d/ic2qizHhtNUHbFa0e86BJpx5sB9XmiTo9PZ3169f7X5eUlJCamhqwj8ViwWKxtHVoEWd3TkW4QwiwcmsBk0bIQ8XOJmH85ehiE30P+1x2FL2RqB7D/CsWy5b8JzBJ6/SYB01E0epR3Q5iBpyDq/wo1ZuXoOgMxI+/HI1GR/HnL+O116KJiSOq2xA0xigsI6ZhSO0apiuNXG2eqM855xxeeeUVysvLiYqKYvHixTz11FNtHUa7UFUdWYtMZCZ15xU7dDKxQyeHfM9ZfDhwg9tFwoSr0McH3oDFj5kV8Lrr/a/hrihCn5SBoo2cB+eRqM3/ddLS0njggQe46aabcLlcXHnllQwdOrStw2gXRg9I47t1R8IdBuBL0hOGZ4Y7DBGBonoOx1Va/7BZn5SJLi4FT20VKBq00aFr1mj0Rrl7bqGw/BibPXs2s2fPDsdHtyvjh2UwsEciOw+VhzsUMlNjgp4lCAGQOPl68Hqp278eQ3I2CeffRMkX86nZtgwAy4hpJF34M/n6OQ3yZCjCPX/vRPS68P83GfTacIcgIpRGbyT5gtvoes/fSb/mt7hKDlOz9QdQvaB6sW78BtuBTeEOs10LfwYQzfphQy4ud/inxk0ZLb+iipZxluQGbytteh6+x15L9bYfqdu/EdUbGe3nIo2M4Ee4D5buC3cIAIwdlB7uEEQb8rqdKIqCotWf9LHRvUdRufxD/ItcFA3RvYaH3NdVWUTBm7/xjWcDUT2HkX7tYzJM0ojcUUe4kgrbiXdqA0+8tircIYg2oKpeSr9+jZw/3UjOCzdTsfxD3DWV1OxYjrO4ZQ+2TZl9SL38QYyZfTFm9SftyocxpIT+jcy67it/kgawHdwiDXRDkDvqCBdnNmBzhH8peX5J5JRcFa2ndscKrBt8JR1Uj5uKH9+hYvkHcKxYUsLk60kYf8UJz2MecA7mAeeccD+vK7jdnBpiW2cnd9QRbtLwyFhgEmcO7p0nOh5H4cHgjQ0q2lUu/xCv88z9lhc7fCo0mEOtT8ogqvuQM3b+jkLuqCOY2+PlyxUhvnHC4IKx0t6jM4jqPoSqNZ83+b7qduKqLMF4mvOfHQX78dRZieo+hMxbnqdm+49oo2KJHT5NFr+EIP8iEey/i3ZRaw//sAcgNT46iejeI0mafitVa79E0RswpPWkdvuPAfsULPgtGTc9jTGt+yl9RvFnL1Oz3TfHWheXQsZNz5A0Ze7pht6hyXdfBFu5NT/cIfiNkVkfnUbcWTPpes/fSDr/Juw5W4PeV502qtYuPKVzO44e8CdpAHdVCVVrvzjlWDsLuaOOaJEzRenbtUfolRUf7jBEG1HdLooXvoK3LnTJUdXtOuE5HEcPUrZkAe7KYmIGnE3i5Dl4QpzPWZpHzY6f0MbEY+o2WKbmhSCJOoIN7JnE0bK6cIcBwOLVOfz8cqnJ0lm4a8qbTNJodFhGXdjs8arHReF7z+CprQSgavXnaIwxxJ99Cbq4lIBmAbYDG7Ed2AhATP+zSbvioTNyDR2JDH1EsNkTeoY7BD+3Vz3xTqLD0MWlok/MCNhm6jaY+HMuJ/PW54nqOrDZ453FR/xJ+jjbwc0oOj0ZNz2DxhgT8rja3atxHI2MB+iRRO6oI1jXdAsmgxa7M/zLalXJ052KoiikXfUIZd+9ias0j+jeo0icchMafcu6DukT0lH0xoA50YZjDx91liTQNH2P6HVGxm+RkUTuqCOYXqdhUM+kE+/YRg4VNN0+SXQs9vy9FH/6EvbDO9AldsFjr6H063+FnmcdgsYUQ8qse9BE+xqAmLoPIWHiNVRv/YEjr9yJ6grdvUifku1vASbqyR11BHO5PWzaUxzuMPxq6k78AEm0f6rXQ9FHf8RT7Suvaz9UP/OjdtcKsu58CX1calOH+5kHjiem31i8TjvaKDPOsnxKFs6nYaNbU9eBGDP7orocaM0JWEZOR9FIpcbGJFFHsEMFViJlaNgSo2dgBN3di9bjqijyJ+nGVJeD2t2riR97cYvOpWh1aKPMADjy9tAwSQMYUrqSdP6NpxVvZyBDHxFszY7CcIfgd+6ILLQamTbVGejjU9HGxDf5vi4mAU+dFVvONrz2lteAMWb2pfGUU2Nmn1OMsnORO+oItisCOrsc54mUW3vR6hStjrQrfk3JV//AVZqPJirG3zHc1G0wKipH/nIHqseFYogibvQMXJVF6BO7EDf2YrSm0DM6DMlZJF90JxU/vovXaccyajrmwee25aW1W5KoI1jv7Hi2HSgNdxgA7D0SWR3RResyZQ8g+86XAajL2U7lyo/Q6IwknDeHwv89ierxPa9QnTYqV37sP85+eAcZNz3d5HktI6ZhGTENT20VVeu/ovSrf2AeMglT9kDq9qzFcXQ/pm6DiO45vFWvr72RRB3BLpvci09+2B/uMAAY0D0x3CGIMLAX7Kfwf0/62moB9tydeO1NT5+z5+7CVVGIPqHpkgOq10PBW7/DVVYAQPWWpcQMGEftzhW+HVZ+TOLUuS0eB+8MJFFHsCOF1eEOAYDYaD1zLpQpU51R7Y6f/EkawGuvxZDRB2dBE52HtLqmF7Mc3Ez1pm9R7XX+JA2A6qV2V2Bjiqo1X0iibkASdQTbFyHDDdV1LpwuDzFRJ9+WSbRvWnNC0Lb48ZfjLDyEPW8voOLI3YXqdgIKCeOvRBsdG3SMdcM3lH79zxZ/rpQ6DSSzPiKYyxP+prbH/bSl4MQ7iQ6lestSqrctQ2m0GrHs2zcxDxyP11aN/dAWVLcTRW8i/brfkTDxqpDnqlz9WfMfpgZ+rce3oItMZyI/tiJYU8u2LdE6HG4VRxsuLZeZeZ2L7fB2Sr74a8j3PJVFlHz5d5yFB/zbVJcd28EtTT4EVDTBqUYTFYvXFji8Zxl1AbHDp2JMj5w6N5FA7qgjWH5xTcjt1jp3qyRpoz70ijCTQcPE4Zln/PNE5LJuXNzs+yEXxDS6K/bUWVGP3W0kTL6u0c7KsXnVgez5+9AcWyAj6skddQQb2DOJZZsDmwcoNF7b5asJ4nKf/jBJlFGLwxX8A8BiNmJoIomLjsfrdlK3f0Oz+5gHn0vdvnU4i3MAUIzRxI6cDvgq5xV98gKu0jx0CemkXfYg5gHj0N/+ApWrPkPRaogbPRPFYKQgbw9ee/0NibPwIKVf/p0u1z/RatfXHp1UorZarVgsltaKRTRy4bjuFJTUsHjNYX8FvVCjIWciSQPExxqprnMFLW4pLrexYksBU8ecXp88Efk8ddUUf/YSqjOwaJIm2kJ0r5G4yvKJ7jOa+HMuI/6cS6jZsRyvvRbzwPHo4lIAji2UyQPAXVFIyRd/Jetnf8aY1p20S38RcN6sO1/myMu3BWyz5+5uxStsn1qUqA8ePMi9995LdXU1H374ITfffDPz58+nV69erR1fp6bVKPzs0iHYnW4WrznS6p+Xc7Tp6YAeb+Q82BStp/Sb17Ad3By0PXbIuSRNvTlgm2KIwjJiGuBrVlv23QK8LntQhT1n8WFU1YuiBI+06szxGDP74sjf699myup3+hfSwbRojPrpp5/m//7v/0hKSiItLY0bbriBxx9/vLVjE4DD5SG/JPRYdVtJjjMxfmjGiXcU7Z4tZ1vQtpgB40mYdE3I/b0uByWLXiX/zUep3b0K24FN4GlUZVGjJe+1B6nbvzHkOVIvvg9jVn9QNJi6DSJ55t2nfR0dTYsSdWVlJePHj/e/njNnDjU14U0encH+3EpufvIbdhwMT82PbumxXDO1Ly89OBlztCEsMYi2ZUwP/C1Zn5RB2uUPojFEhdy/fMl/qN64uPnOEl4PrpJcX+nUuuDf2vSJGWTOfYaev/2AjBv+H/r4E5dQ7WxaPOvD4XD4m06WlJTglV+FW91/vtpJjS18NaAPF1bz/pK9vLVol//pvejYki+8HWOX3oAvgaZc/Itm96/bt77F51bdThxH96OqKo6jB3CVy9z8lmrRGPX111/PbbfdRllZGS+88AJffvklt99+e2vH1umVVtnCHQKqCt+sPszE4ZkM65MS7nBEK9MnpJN56/N4nbYm76ID9k/KxG1tVDhMqwOPO8TeCsWfv+KbO31sKl/ssPNJmXXPGYi8Y2vRHfWVV17J/fffz+zZs3G73Tz11FNcf/31rR1bp3fuiKxwh+B3tLTldYdF+2XP203xZy9TtvgNnMUnfoCdNO1mdPFpAGiMMSRdcDuJ5zaeM32cireuKmC+dfWWpdQd2HQmQu/QWjw9b8yYMYwZM6Y1YxGNXDWlL1U1DhatysHtCd/Qg0aBUf3Twvb5om04inIoeOsJ8Pruhmt2ryL7539BF6LeR/WWpVSs+Ai8XixjZhHTewRaSzIane9Zhj4xA9vh7dTuXtVkt5jjCj/4A2mX/YqYfpJfmtJsou7fv79/XDqUXbt2nfGARD2Hy8PS9blhTdIAioKMUXcCtTtX+JM0gOqoo27vOizHFrL499u9OmB5efm3/8aY+nv0ifUzg2L6jSGq+xBqtv144g/2uCn65AWSptxE3FkzT/9COqBmE/WqVatQVZWXX36ZzMxMrrnmGrRaLR9//DEFBfIgoLXtz6uk1h5qrK9tebzwp7c3MO++ieEORbSiUJXyNI26taiql5KvXg3az5aznajuQwK2FX/2csCqw2Z53JQt/jeKVh/0g0GcYIw6ISGBxMREtm/fzh133EFcXBxms5mbbrqJtWvXtlWMnVbXtFj0usgox7IrpxxniOXlouOIHXaef8bHcaWL/omjKMf/2lVeiNdmDTrW2CVwWp/XaQsxI+TElb1q90heCaVFWcBms3HwYP1qoz179uByhW/aWGcRZzbyy2tHEG82tuBLvHWlJUZLvY8OTmOIIqHR6kOvvYaKZe/6X+tiE1GM0QH76JOzgsaXFZ0BTaO61Iqu0S/wmuCvJ12CPAsJpUWJ+pe//CXXXHMNN9xwA9dffz033XQTDz/8cGvHJoBJI7J484kL6Ns1+NfStqIAj9w0OmyfL9pO9ebvgrY17MaiMZhImXmXv8KdIa0HXa57LOgYRaMledqtKFpfswmNKYa4sZfQ8K46YcKV6JMCV7zW7V1L0Sd/xnmsVojwadGsj+nTpzNq1Cg2bPBV1Bo9ejSJiafXQ++ll15Cq9Vy3333ndZ5OjqHy8OPG/M4XBj862ZbuXJKb/pkh+8HhWg7upi4oG2mbr6xZ3dtFZXLP8BReIjYEdOxDJ+CLi4Ft7UM1etBaXSHbB48kaiew3CW5mJM74XGYCJmwDjsh7djSO+JIbUbFcveCzjGU11O7c4V2A/vIPuev6Fp1LSgs2pRonY6nWzcuJG6Ol9Ty++//54jR47wwAMPnPQHVldX89xzz8mimRZ64p+r2HGw7KSO0SjgPUOTNKJNWq6eIkVyOgvLqAuo2rwEGjwErN60GNvBTbgri/zbHHm7cVcW4sjfh7uqBG1sEmlX/BpToxrT2mgLUV0H+V8b07pjTOsOgOpxo4m24K0Lvgnx1FZiz91NdM9hZ/gK26cWJeoHHniA3NxcSkpKGDhwIFu2bDnlOdVLliyhe/fu3HLLLU3uY7VasVoD//MKCwtP6fPas71HKk46ScPJJemUeBM1Nhc2R/CDQoNewx/vm4TJKGXLOwt9fBpJ515D2Tev129UvQFJ+rja3WvA6/u68VSXUfrV38n62YsB+3jstSiKgqbRuDZA7d616CzJOG01QU0HQJGaHw206Dtw165dLF68mN///vfccssteL1efv/735/SB1566aUAvPLKK03us2DBAubPn39K5+9ImpnCDsCl5/bi0x8PNL/TCagqIZM0gNPl5YMl+5hzYX/Sk0J3lhYdj7Mkt2U7Nqr34yypH1dWvR5KF/2T6i1LQdEQd9aMgDKpNbtWUvzxC/7Xmug4NDFxuEuOgKIhfsKV6BO7nNZ1dCQtStSpqanodDq6d+/O3r17mTFjBtXVTdcuBli0aBHPPfdcwLaePXvy5ptvnvDz5s6dy2WXXRawrbCwkDlz5rQk3A6jT3YC8bFGKqsdQe/FmHTU2V30yLBwqODUx69Lq+zNvv/Dxjx25ZTz6qNT0GojY6qgaF2mrAG+injN0ETHYUjJxn54u39bdO+R/r/X7l5d/2BS9VK1ZiHRvUYS1WMoADXbfwo4n7euirQrfoXWFIsmOjbkasjOrEWJOjo6moULF9K/f3/ef/99evbs6R+vbsqMGTOYMWPGKQVlsVikk8wxl0/uzb8X7gjYplGg1l7fTECrVfC04urFovI69uVW0r/76T1AFu1D6NXICorRhC42GcuoC7GMmILX6aB86VvYc3dhSO+JotGQ++ovMGb0RWMKHuoo/OiPxI+7jPizL0YX2/hrSUEXm4Q+Ib1Vrqm9a9Et0uOPP86uXbsYP348Go2GG2+8kdtuu+3EB4rTNntiTy44uxtarUKUUctF43sEzWf2eNQTDpOcrjiz1KPuNLTB85vjx19Bj1//l+w7XyJu9IUoWj3aKDMpM+8iadot2I/soGbbj7hK86jZuhRH/r6gc6iOOip+eJuij18g/pzL0PmTsoJl7GxsBzdT9MmfqVr7BWrI6nudV4vuqD/66CP/vOmXXnqpNeMRjei0Gu69aji3zBqEQa/lcKGVr1YcCtjHV4sj9PHNvXcy1u8qZvZE6Q7dGUT3GY0htRvO4sOAr18iGg3lP75L7JBJATU97Lm7KHz3GRp383QU7CN59r2UfvWPoJKndXvX4j3/RrJ//hcc+XvRmhOwbviG0q9fA3w1R5wluaTMvKt1L7QdaVGi/uGHH/jVr351Rj9Y5k+3jMvt4aV3NrF8Sz4mo47JI4NLn+p1GhTA4Qpu5tC9i4VeWXEUl9k4kB9cO+SsgWnMnNCDfUcqWb+zCLfHS35Jjb+Z7nG5Rc0/kxAdh0ZnIOPm56jdvQqv3UbVui+o/Ol9AKrWfE7mLc9jSMkGoGbHckK1XNYnZWDK6IMhKdOf8BtTNFpM2QMAqN66NOC96m0/kHzRnSH7LHZGLUrUWVlZ3HrrrYwcOZKYmPqn/81NsRNnxpcrcli2OR+AOrubRStzgvZxurzERhtwuJxB7x0qsB5LsgreELfWBp2WEX1SGdUvjWun+eZLF1fUceez39KwuXmUTNHrVDR6I7FDJlO7Zy3uivqpsarLgXXzdyRP833v6yxJwcdGxWIeMpm8fz4QYtodRPcdgyEpA9XroWrtl9gObQ3aRxsVK0m6gRZ998XHxwOQn5/fmrGIEA4VVAW8VgGdVgkqfappZozat2/o8Y8VWwtYvf0oI/qlciCvkq7pFlITohnWN5UNu4v9+y1adYhrpvUl2qQ/1UsR7ZCiC/7/VrS+tKF6PbjttSiGKFSnrxtRdL+xpF3+Kw6/eEtQkraMnoE+MZOoY4tYKn58h8qVnzQ8M6CCoiHx/Btb5XraqxYl6ilTpjB16tSAbZ9++mlrxCMaGdYnhaXr6+e16nUa5lzQjze/DKwFnpoYTVVt8B11S3y39jAvvL0Bp9uLXqfhgetGUl0XeC6bw0NJpY1u6ZKoOxNjZj/fMvGqEsA3Xm0ZeQGO0jzy//XrwI7jipa6A5s4/Jc78NqDOwLZcndhXb8IgKjuQ7AfDV4DkHLJL4jqOhCdJbl1LqidajZRL126FLfbzbx581BV1V883u1288orr/gXr4jWM3F4Jj9tzmP7gTK0Gg16nYZlmwqYMjqbjXuKqbG5cLm97MutbPE5Gz9gXLer/s7Z5fbyyvubGdYn8BslLTGarNTAamii4yv68A/+JA2+cqa1O5dTs2dtYJIGUD3g9uB1h7hh0OpxNSiXasvZFmIfHeaB44NqhogTJOpdu3axevVqysrK+M9//lN/kE7HzTff3NqxCeAfH29lvT+R+h7wVdY4OFJk5ak7z+E3f1tx0ucc0iuZkkpbk30QbQ43q7cXYo7SE2c2kpVqZu7MgWibG18RHY6zJBf74cA5/LYDm7Ad2BSyRGmTdHpwt6AssteD11GHNkpuCBprNlHfc8893HPPPbz99ttNrgr84osvmDVrVqsE19mpqhow7NGQ26NyqKDqlKbfHSms5uJJPfnPV823UquxubjxogFcdE6Pk/sA0SEozVWu87asiYRl9EXUbPsBb6hE3TiBq15q96zFMnzKyQXaCbTosWpzS7dff/31Jt8Tp0dRlGYXmgztk8KF47o3+b5WAwZd8F1wVqqZnhlxtGRFuMsd/NRedA76+FRih09t+v20wB/gWnPgakNDWg8SJ18X8jF2TP+ziTsr+AZPa5KaMqGc9pwraXraum6dPYgX/rcRr7d+9aHJoGVwz2SWbcpn1vgenDsii3lvrafcWl+34/jMEE+jUnrxZgP7civ5/b9WY47S4/J4cThD3x3Fm41MGpHZatcmIpezJBd77i4so2dgHjwJR/5eKlZ/imrzlT/VJ2bgKgpceOW1VdPtgTdxVRxFdbswdR2AomiIP/sSKn58x79f/MSrSJx0LR5bNXV71uAq9zUmMGb1J7qPNKgI5bQTdXNdysXpcbg8fLUyB++xZJuZHEOfrgls2FXEumN/3v9uL1NGZ3PvVcP4w4J1ON1etBqF0QPSWL09sDTsrPE9+GrlIX8Z1BqbK2jcWQGuvaAfWo3C1LO6khBraotLFRGkeuv3lCz8K8endMaNnY2j8CC6aAuG7kOJGTCO4s9eDjpO9bhAAVNmXxxHD1Cz9QeiegwlYcKVGDP64Mjfi6nrAKK6DQZ8c6WzfvZn6g5uRtHp8diqKfv2DYyZfTAPniTzqBuQVQwR7MeNeQH1qPNKaskrCX4AuGR9LuOGdOHfj01n75EKembGsSunPChRe1Q1qFZ14zvuCcMzuX56/zN3EaLd8XVdqf+6qFqz0P93V1kBxozeQcvCj6vduw53VYl/JSNaHelXPUp0rxEhmwAoOj0xfc+i/Md3qVz+gW/jhq9xFuUElEXt7ORHVgQrO0EJ0oZyCq3EmY2cNTCdpLgozhmSwQVnd0OjUdBpFS6e2JP+3ZouHWnUa7lqSh9+ce2IMxG6aKdUVcVjq2l2H0fBfqK6Dwn5Xtk3/6Jyxcf1GzxuKo4n4GZYN34T+HrDNzKs2sBJ31GXlJSQkpLify3/mK1n/NAuvP/dXtyeEz/QG9HX1w3D5fZirXVwtLSW2y8ezM0zB6IoCjFRehwuD58uO8Ch/OD61Q6Xh9hoA0a9lhqbi017iqisdpCZGsuwPikyNa+TqPzpA/8qw6YYUroSN2YWJV//k9rtywLeU13BtdNVpw3V66Fu/0a8tmqi+5yFtlGHco3ehJf6r0vFYJJh1QZOOlHfcccdfPJJ/bLP2bNnn9GARL2u6RaevWs8//tmF5v3lTa5X6LFRJekaF7/fDsLfzroH84wR+m558phjOzvS+JGvZb/97Nx3PTkNyGn9Lk9Xg4VVPGbv62g1lY/bapf1wSevXt8UHlV0fFYtyxp9v2oXiOIGzsLjSGKlBl3ULv9J4LKEzSaMxo74gIK33nKv8hFMcaQecsfMDToQJ5w7jW+cfFjy84TJ11zZi6og2g2UY8cOTJom91uZ8SIESiKwsaNG6UudSsb0CORa6b1azZRl1vtvP75DpY0mnNdY3Px/FvrMRm03HHpEKaN7caWfaUhk7ROq3DuiCze/HJnQJIG2HOkghVbCzhvVPYZuSYRubQmMx5r6K+1pGm3EDemfkqdu7qckDVkVJWonsPRJ6QT3WskitFE2Tev1b/tqCX/jYfJuv0F9PFpAMQOmYwxow/23N2+qnupXc/odbV3zY5Rz5s3j+TkZF544QUWLlzI559/To8ePfjiiy9YuHBhc4eKM2hgj6Rmiy4B7D5c3uR7dqeHf3yyjRqbi5q60PVA3B6Vj3/Yx4G8ypDv19S1YGWZaPcSJ18P2hD3b1odlSs/Ie+1B6jdtx5nSS5l377Z5Hmie48i+cKfEd1nVMi6H6rDRvn3bwdsMyRlYhk+RZJ0CM3eUU+dOpXevXvz8MMPc91113HZZZdhMBjIzJS5tW0p5+iJeyIWljXfGs3p8lBaaWNEv1Q0SuhO5V+uyAl5rFGvZfywjJDviY4lus8out7zD+x5u1BdTur2rcNdXYEjfw+e2ko8tZUUfTgPbXQcnprQNweG9F7EDj3P/7ruUIi6HviaC4iWOeEYdffu3VmwYAFPPPEE69atw+2WFjltaeehMv7v7yuDEmvjZOvxqozqn8qewxV4vSoGg4bK6vq75y5JMXRNi0WjUfjdrWP5z1e7WvQDAOqTfKJF5lR3BrrYBMwDzgEgduhkij97GUf+nvodvJ6gJG1I7Ubi9NtQFAVTdv+AOdD2g5tDfo4+KbgJhgitRQ8To6KimDdvHm+//Tb79slPwbb01qJdIWd9nDsyi+835PlfaxTFXz/aoNfy+E1j2HGwjJXbCuiSFMONFw1Ac2z85KyB6Zw1MJ0t+0r4auUhrLVOth8oC/qM41Rg455i+naVztCdkbFLL2oCZncovr6KDeZSm7L6E91tUMjj9SnZ/tWHfjo9yRfUP9+qWP4hlSs/RnW7iOoxjPSrHglZC7uzOql51HPmzOGDD048J1KcOZXVwdOdNBqwxBi4cFx3oow60hKiA7q3OF0eFny5k0MFVfTvlsjNswaRkRzc73BYnxRuumggPbpYsMTU1xSJMgbP7uiWLl3hOwNVVandvZryH9/Fnusr2mUZdQHmoeeBRosmKpbki+4k+YLbUfS+37CMXXoTP/GqJs+ZNHVu/bjzsSl3iqKhdt8GAGy5u6n48R3f1D7Vi+3gJsqWLGjFq2x/FLWdTITOy8tjypQpLFmyhKyszvMr0wdL9jZZ5e7W2YO4bHJvjhRaueeP3zd5jqQ4E//8zdSg6XVlVTbu+eP3AbM8NIrC+aOzOFhg5WC+r7tMRkoMf/31eeh0Mj2vozv6/nPY9q33v06eeReWY4WZVLcLNBp/vWiv04anrhp9fGqLzl300R+p3b06YFvm7S9gXb+I6s3fBWzXJaTT9e6/ns6ldCiyMjHCXXl+H+6/ejiDewX3ptu811fQvWu6heF96xchNZ4gUlZlZ+v+4ClXK7YUBE3F86oq363L9SdpgIKSWn/fRtFx2XK2BSRpgPIf/uf/u6LTBxb1V1Wq1n1J/huPUrZkAV5n8ytp7QX7g7aV//R+yFkhxjQprduQ1PqIcIqiMG1sN8YMSueWpxYHlB3tkVE/HPHYrWP5fkMexRV1lFbYWLohcE51QmxwbWFzdMvHAA8flS7kHV1Fw6Xfx3jtddTuXoMhvbt/zvNxJV/8jdrdqwDfDA53TSVJ581Ba04I2aXFmNaNukZztG171gTtp4tLIXnW3adzKR2OJOp2Is5s5L6rh/Ovz7ZTXedkVP80rp7a1/++Qa/lgrO7Ab4FMLsPl1NwrIPL9LHd6JUVH3TO8cMy+WL5oRa18Ro9IO2E+4j2TaMLUfvc46Loo3mgaEi+8GdYRk7HWXwYj72W2kZJtnb7T9RuX4bWnEjqpb8kqtHDxZSL7yfv1V/iqakI/flRFjJvm4c+LiXk+52ZjFG3Mx6PF4fLc8Ju4B6Pl52HyrGYDc0+CPR4VbbsK2HNjqN81WgetV6noWt6LLMn9GTKWbIIoaOz5Wzj6NtP0lTHek2UGVOPYdTtPNb+TatrsoqeLiGd7Lvmh6zXUbbkP1St/ixou2KIosdD/z3l+DsyuaNuZ7RaDdFaDS63ly9XHGJfbgVDeiUzfWw3//S74/sN6e1rUJtbVM3fPtrCgbwqhvdN4Z4rhxFn9g2FaDUKI/ulMrJfKrtzKgLGpmec052fXRK6SproeKK6DyG6z2jq9q0L+b7XXlefpAE8bhSdAdXtxPdkpD7BuysKUd3OoHZeXpcD68bFIc9vSJESBU2Rh4nt1N8+3MLrn29n2aZ8/vrhFt5a1HT/w3lvrWf7gTJsDjerth3lHx9vDbnf/908hgnDMshOM3Ppub2Ye9HA1gpfRKio7oObfM+Y1S9om3nIuWTe/gLmQRMC39DqKVv8b7yNKvGpLmeT1fkarmYUgeSOuh1ye7z8sDHwYeF3a48wd2ZwYq21uYJWIC7fUsCsg2UM6hk4kyQ1MZpHbjrrzAcs2g3LyAuw5+2hdtcq0BmI6T0SRavD0KUnsYMmkfvaA3jrjn09KRpfMaW07iTPuAPFEEX11qW+4RCPyzflTtGQctGdeB02qrd+j9dWg6nrQOxHdjb4VAXz4InEDjs/LNfcHkiijnC7D5fzzjd7sNY5mT6mKzPO6YHmWH3pqpr6JeKWJprgRpt06HWaoCa1r7y/iX882nTjUtE5KTo9aZf/Co+9FkWrQ9No6CLjpqepWv05XqcNy/CpmLJ93YA0xmgSJl1D9abAYQ3boc14PW7y33gUV9mxlbRaPbEjpuO1WTF1G0zMoInoooIXZIl6kqgjWHWdk8dfXYXN4Xtgsz+3EkuMkfHDMrh55iBe+WAzXq+KXqdpcphCUZSgedUA+SW1eDxetC1pRS46naa6gRuSMkmZeVfANnd1BZ66KvTJWWhjE/FU19cB0cWnk/vXu/FUNyhR4PHN3U+74qEzH3gHJYk6gm0/UOpP0set2XGU8cMymDqmK0P7JHMgr4r+3ROabEKrqir9uycGLXgZ2S9VkrQ4Ja6qYsoW/xtH4SG0JjPO4sOAiiGtB0nTb6Ps2zfwWEsxdumF6nYEJuljlFClVEWT5F8rgmWmBP86mJ1W38IoNSGa1IToJo+vs7v4v7+vYH+ebyaHRgHdsdkgD1wX3BRCCK/biSNvD7qENPRxoZeGF3/8Z3+J0oZNBpxFh3Dk76HrvX/Ha6/FUbCfwnefCTpeMUZjGX1h61xAByW3VBGsa7qFORf2R3fsznd43xRmTejZ4uMXrznsT9LgK4vqdHvZtKeYjXuKz3i8on1zluSSO/8ujr79e3L/eg+VK0OtVKxtto60u7IYRdGgMZkpXfQqjedk6+LTyP75KxiSpKb9yZA76gh37bR+zBrfA5vDQ0pC1Ekd21QXc68K//tmt7TWEgEqfnoPT22l74XqpXzZe8QOnxbQiFYxRqOLT8VdGfoHfUz/cb6/eNy4qxrVl9HqyLjpaXTm+DMffAcnd9TtgDnacNJJGqBremyT79kdntMJSXRA7upGHVs8bjx1VQGbFEUhZfZ96I4t89YnZRLdezSmboNJmXWPfz61p66KqJ7DAo6NHTIZXWxiyM/22mup3rKUmh3L8bpDt4vrzOSOugNr6gEjwEXndG+7QES7EDt4Eo68+k4uhvReGJKDyzVEdR1I9j1/w2urQRsdWJ7AVVFI4Qd/wFVybJ6/oqA1J2AeNJGEBp3FVa+H6s1LsOfvwZCcTdW6r/wPHQ1pPci8+TlpHNCAJOoObFifZNKTov39FBUFzh7chYnDM5k4XMYIRSDLqAtRdAZq965Fn9CF+HGXNrmvomiCkjRA2Xdv1idpAFXFU12ONjouYE522XcLsK77MuS5nUWHqN27FvPA8ad8LR2NJOp2xOPxsmV/KUa9loE9EkMWvGm47/YDZdw2ezB7cyuornMRG6Vn9Y6jvPvtHtwer4xRiyCxw84ndtj5eGqrKF38OvYjuzBl9SNp+m3oYk/cis1Zkhtye8WKD4kbO8tf/rR685Jmz6N6pOt9Q5Ko24kam4tH5v/EkUJfXejhfVK4dlpfvt+YhzlKz+yJPUmK841j19pcPPrX5f6l40N7J3Pt9H789m/1BXVefGcj2Wmx9A5R/lSIki/+St1+X6us2t2r8DptdLnusZD7qh4XZd++Sc3O5TT12Et11FH2/duotmo05gSaqtAHgFZPTN+xp3kFHYsk6nbi2zWH/UkaYPO+ErbsL+F4kdqfNufz90emYNBr+XbtkYD6Hlv3lxLXaIm5qsKWvSWSqEVItkNbm32tej3YjnUXtxccwLrh6/o3NVrwBj+stoYobRqSx4XHVo3GePIP0DuqNk/UGzZs4LnnnsPlchEfH8+zzz5LZqaMl56ItTb4SXjDSuLFFTY27ythzMB0rLXBDXFjY4JrgfTIjDujMYqOw5DeA0f+3vrXqd38f/e6HBQs+D+cRYcAUAyNHlqHSNIny3H0QIt7MXYGbT4976GHHuLpp5/ms88+Y/bs2Tz99NNtHUK7NHlkFnpd/X+XQRf8X2eO0rN+VxFer+pfJAMQG63numn9uHhST3RaDQadhivO683IfvKNIEJLuegu9MdmfOgSu2Aeeh7WTd/hqa2iducKf5IGUBv3StSe/mwNnXR5CdCmd9ROp5Nf/OIX9O/vq7jVr18//vvf4I4OVqsVqzWwNGdhYWGbxBipunWx8Py9E/hm9WEMei3jh3Xhhbc3UlLhq+07bkgXlqzLZfGawwBoNTC6fypdUszMGt8DULjy/D7cOGMAiqJg1EtHcdE0Q2pXsu98GY+tmuLPX6H8238DUL5kAeYR04L3T+mGszQXrTmBpOm3UrdnDTXblzV5fkVvQlUUdKYYovudjXXdFw3e1KAzn/jBZWfSponaYDBwySWXAOD1epk/fz5TpwaX2lywYAHz589vy9DahT7ZCfTJrv8C/vsjU9i8p5jYGAPpSTHc8v++8b/n8YLL4+X2iwcz/4PNLFl3BBU4b1Q2918zIgzRi/aodu96bMceKgJ4HXUBc60B0GhJv/a3aGPiQaNFURTM/c8mYeJVeF1OFEVD5erPqNn2g/8Q1eW7C3c7bdhzdwaeT/VSt38DlpHTW+ei2qFWS9SLFi3iueeeC9jWs2dP3nzzTZxOJ48++ihut5s777wz6Ni5c+dy2WWXBWwrLCxkzpw5rRVuu6TVKERH6Yk3G/F6VbyNHqR7vCprdxby7doj/m1L1+cyZmA644dltHG0or05+t6zAUn6uIbDHsCxMWklqCKePrH+a6y5qX3OwoNB23QJ0ky5oVZL1DNmzGDGjBlB22tra7nrrruIj4/n73//O3p98HiWxWLBYmm6IauA4oo6fvO3FRSX+xaz9O+WwJhB6azd4RsiUhS4eGJP8oprgo7NK64O2ga+zjGLVuawP6+SIb2SmHJW12bnaouOq+TLv4dM0gCqq9HDakVzwrKlMQMnULl6IXhDN8MN2HfQBKK6D21xrJ1Bm8/6eOihh+jWrRtPPvkkGo2UGjlVHy3d50/SALsPV5CdauaB60ZQUFLLmEHp9O2aQM5RK28t2oX32O22RoHRA9JYu7OQf36yjbIqG5NGZHH3lcN49eOt/rvvpetzKSyv44YLB4Tl+kT4uKsrqN7S/IKUhgxp3dHGhJ5BZDuyg+pN36EYTCTP/Dnl3y3Aawt9owCARkvKjDvlBqGRNk3UO3fuZMmSJfTu3ds/tJGamsprr73WlmF0COXW4Mp4ucU1dE2zcP7orv5t3btY+O3cs/jkxwOoqsql5/YmLSmGR/66HIfTN41q6fpckuOjWLo+cFXZt2sOS6LuhDy1lYFzP0/AWZSDp7YqKFnb8/dx9L+/B9XXBq66ie7jDZmHnBvUuVy0caIeOHAge/bsOfGO4oQmj8pm9fbgmTDm6OChpLGDuzB2cBf/6+0HSv1J+rh9RyqINumprqufr22ODt2HUXRshrTuGFK74iyuf7YRN+5SnMVHsB3YGHyA6sXrqAtK1DU7lvmTdLM0Wgyp3XFXFlKzZSm2g5tJu+JhTJl9TvdSOgwZe2inxg/N4GeXDEanrf8V8aJzupOeFMOSdUf49cvLeOwfK9l+oDTo2J6ZcUQZA6fnDeqZxNyZA9AcO51OqzDngn4cKqjC7jjxuKLoOBRFIf26x4kbM4vovmNIveSXJJ1/I2mX/wrFELxaUDFGhazxoY1p+RQ7r82K114LgKe6nLJv/nXqF9ABKap6Er/jhFFeXh5TpkxhyZIlZGUFl17srOrsLrbsKyElIZreWfFs2F3E719b7X/foNfy2m+nkmgJXD22cXcxr322jdJK3xj1zy8fgl6npbCslv15lZgMWl55fzPlVgcxJh0PzhnFmIHpbX15IoJ4XQ4K3noC59FQHV4UMm77I96acpxl+UT3Gok2Jo6Ctx7HVXIkcE+9MfiBZOOzGaPp8eu3zmD07ZvU+mjnok16xg2pnwa1ZkfgcIjT5WHz3uKAcWuAkf1T+Xv/KQDYHW62HygjM8VMelIM6UkxPPzKT5Rbfd9MtXY3f/twC6N/l4ZGIw95OqvqLd83kaQBVIreexZPja/5QPnS/5J+1SNk3f4n6g5sxrpxMY6CvRhSsjGkZGNd/3XA0absAdhzd/lfx/QZ3VqX0S5Jou5gslKDG+JmpTbd6eVAXiWPvbqK6jonGgXmzhzE5ef15mhZbcB+ZVV2nC4PJqN8yXRW7srmVwcfT9IAeD1UrvqM6N6jiOkzCq+tGldZHvb8fdgP7/DvpuiNJEy8mtjhUyj//n84CvZh6jqAxHOvb63LaJdkjLqDueDs7pw10LdYQKtRuOK83vTt2vRY4X+/3u1/gOhV4e2vd1FrczFuSJeA/Ub2T5Uk3cnF9BsLtPw3KvVYcSZnyRFKFs7HXVEIjdpsmYdOJn7cpWijYok/ezbJM+4kadqtUjmvEfnO62CMei2P33Y2pZU2DHotlhBV8xqqrA6c5ud0e6mxubj94sGYo/Rs3VdKr6w4bpgh0/Q6O1P2ANKueAjrhkUoOgOGLr2o/On9+h20OvAcf/CsEDd2FgD2Iztpqv60q+wo4FtgU735OwAMqd3pMuf3AU11OztJ1B1UcnzL7kjOG5XN/rz6BqYDeySSlhgNwE0XDWyV2ET7o3o91O5di7uyhOQZd/iXh+vMCVRv/QFtjIX4CVfhLi/AWZpPdO9R/ul1xi69mzyvPW83NTtX+pM0gLM4B+v6RSRMurp1L6odkUTdyV08qRfRJj1rdxaSlWrm8slNf1OJzqv4kxep3b0KgPIf3qbL9Y8T1XUQlpHTA4sndekVdKwxozeJU+ZS8eM7qI07jLudFH/2UtAxbmvwtNLOTBK1YOqYrkwd0/XEO4pOyVV+1J+kAfC4qVqzkKiug1p8jvizLyZu7CyKPvoTdXvWBL7p9YBOD+7jfRIVYgZJY9uG5GGiEKJZIZdaeFuw4rDxeZwOTBl9UXTBz00URYNijMaQ3pO0qx4husewUwm1w5I7aiFEk1RVBa+HqB7DsR3a7Nuo0RI3Zlbo/b0eFI0W1ePGUbAPXVwKOksyttzdFL33LF7HsWmfihJQT+T4Ahhn4SH08VLitDFJ1B2Ux6uyeW8xtTYXPTLi+M9XO9mfW8mQ3snccekQqeMhTshVWUThu0/jKitA0RsxD5+C3pKC6nZR+u2/0RiiSJh4NfqkTKrWfkntrhV4aquI6jEUZ0kunuoyUDRYRs/wNb9t2EtRVYnucxYeew2OBgtdQMV2eDuGVBmKa0gSdQfk9ao8/upKtu73PZDRaRXcHt/dy/cb8vB4VR66QVZ+iaa5ayoo//5tXGUFgO+Ot2bLUsxDz6emQQnUwvef9c28a1B86Xh3ct+BXqzrvwpZjc/YpSf6pEyKAxI1GEM8kOzsJFF3QFv2lfiTNOBP0sdt3lvS1iGJdsJTV03RR3/EfmQHKI0eYalqQJIGWjZWHWqMW6PFPGgiuoR0LHl7fCVQtTriz7kcU1a/U7+ADkoSdQdkdzZf7a5HhnTPEaFVrPjQl6ShZSVKW0Cf2AVX+dHAjV4P+W88Qvr1vyd5+q0kTbnR1ylGI02XQ5FZHx3QyP5p/kUrjWWnxXLXFfJEXYTmClGutDFdfJovqeoMGLODV6zqk7NJvujnRPcehWX0DNKvfwIlKvjmwGuvpfDdpwFQtHpJ0s2QO+oOyKjX8sf7J/LgS8sorbT5tyvAs3eNJz5WOmiI0KJ6jcB2aIv/tTYmjthRF1L50wegetElpNPl+sfRGGNQtDo0BhOFH/2JumPzrHVJmXS5/nF0sYlYRkzzn8eQnIkj1xr0ed66KhxHD2Ls0rP1L64dk0Tdji3blMeCL3dSXedi2tiu3Dp7MNpjZUgTYk3ccelg/rBgnb87+flnZUuSFs2KGzMT1WGjZudydHHJxI25hNJv/ukfBjFl9g2aPpd+xa9xW8vw1FZiSO+B0mhs211dgSN3d5Of6XXamnxP+EiibqeKK+p44X8b/U1rP192kOzUWC4c192/z7ghGfz5l+eyflcRWamxnN2oIp4QjSmKhoRJV/vrbJQtfctX9e6Ymu3LiDtrJsaMwFIDit4YMkn73iRo3vRx+qRMTNn9z+g1dESSqNupvUcq/En6uF055QGJGqBXVjy9suLbLjDRoXhqq5rd5qoqpuijP+E8egC0OmL6jiFl5l1ojPXPSHTmBGKHnheys7nX7cLrclK18mNsR3ZiyuhNwqRrAo4X8jCx3eqbnRDUbaV/t5b3qBOiJWKHnEvDGtTa2CRMPYb4X5d9+6YvSQN43NTuWknx568Avi7ktsPbUb0ekmf+nLSrHkWflBlwfk9VMcWf/JnKlR/jyNtN1dovKF44v9Wvq72RO+p2KjUxmgeuG3lsjNrJ9LHdmH5293CHJTqYqO5DSL/+MWq2/oAmKpa4sbPQ6Aw4S/NQNFocR/cHHVO3bx1H330a24FNABhSu5Fx41PE9D2Luv0bcZXlB+zfuLN53Z61/qXowkcSdTs2eWQWk0dKo1/RuqJ7DPMXSVLdroAkrIQYotAYY/zvAziLD2PdvIT4sy8mfvxl1O3f4FteDqDRBi4tB1DAWXwEY3qP1rmgdkiGPoQQAVRVxXZkB7bDO1AbLXqp2fFTQBJWHXXBJ9AG3wl7aisB392yv7eizhicpH0BULHs3VOOvyOSO2ohhJ/X7eTo20/iyPNNpzNm9qXLnN+j0fumdboqi5s9XjHF4G38AFJRMA+agNdpp/yH/9XP/nA7QKsHjyvoPJ6aitO/mA5E7qiFEH61u1b6kzSAI38vNTuW+1/H9BsbWANEq/etTlQU0GgxpgUvXDF1HYSr/Cgua6m/nKn/cEsihi69fEMgDZgHTzpDV9QxyB21EMLPUxdq9WD9NmN6D9Kv/R3WdV+CRoshtRuVyz/wval6sB/e1qjJLdgPb8d+eDuaEMvIFZ2pftaIRosxsx+WYecRO+z8M3th7ZwkakF1nZMfNuShqirnjswiziyrFzsrc/9xVCx7D9Xp606v6I3EDBiH21pGzc4VaPRGzIMnEd3T93Cx9Ns3gs4RN2YW7spiXBWFOAsP+rd7bcE/BNwlh+tfeD1oo2MlSYcgibqTq7G5+OWff6C4wreM96Pv9/HSg5NJiDWFOTIRDrq4FDLnPkvV+q9B9WIZPQMUhbzXHsRrrwGgav1XZN72RzQ6A1FdB2Fd+0X9CRQNsUPPw5CcRdXaLyhrkKhbovHQiPCRMepObsWWfH+SBii3OvhhQ14YIxLhZkjtRspFd5Iy8y5qd68m/83f+JM0gKs0j7r9GwCI6TeGxPNvRBubhD4xg5SL78OQ7JsyGjNwPJroBsMdhqigz9JEmRu8UrCMurBVrqm9kztqEURRlBPvJDq8o+8+E7QY5TilwT1e/LhLiR93adA+OnMCWbfOw7p5CaASO3wKjtw9lH3/XzxWX2MLr60GxRBF7NDJmAdOkLofTZA76k5uwrBM0pPqFy0kxZk4b5QsounsvC5HwHzphgypXYnuPbJF59HFpZB47rUknnsd+rhUzIMnBnUYV502oroPlSTdDLmj7uRiovS8+MBkftqcj9fjZeKILCwx0vhW4Cvx0ajgXcKUm4gbOR1Fpz/l02rN8S3aJupJom5nKqx2XnpvE1v2ltAjM477rx5Oj4y40zqnOUrPjEZV90TnptEbMQ+aSM32Zf5tUb1GkHD2JUH72nN3U3dgE4bUrsT0P/uENTosoy+idtcqXOW+xrnmIediyux7Zi+gg5FE3c68+sk2Nu72rQ7bn1vJH/+7nr89PCXMUYmOpmrdV9Qd2ARaHbr4NOLPuRx9fCrly97DmN6T6D6jURSF6m0/UvL5X/zHxQ6bQsqsu5s9t84cT9adL2E/shNNVCzGtO6tfDXtnyTqdmZXTnnA69yiGmrqnJijZbhCnBn2wkOULX7d/9pdlo/jyE5KF77i3xZ39sUkTZlLVcOpeUD11u9JnHIT2mOzORxFOZQvfQt3ZREx/ceRcO61KBotikZLVPchiJaRh4ntzIDuiQGvs9PMkqTFGeOxVVP0/nNB22uP9UQ8zrr+a1S3K3iYQ9H4Zw2pHjeF7z2D7eBmXOVHqVz5MZWrPm2t0Ds0SdTtzJ2XD2FU/1R0WoU+2fE8dMPocIckOpCqdV/VlyBtSBv4y7fq9YACsQ0a2AIYs/qiMcUAULHiIzzVgb8BNjWTBHzdYqrWL6J23/qgqn2dnQx9tDMJsSZ+/7Nxrf45NTYXG3YVkWgxMbhXksyt7iTcVaVB22IGnEPdvg2BG70eXNYyKn58J2CzI3c3bmsprooiKn96P+hchtRuIT/Xnr+Xo/99AtXt9H3mwPGkXfbgKV5Fx9PmiXr9+vU8++yzuFwuMjMzef7554mLO71ZC+LMyiuu5uFXllNd5/ummTAsg0duOivMUYm2YB54DjVbl/pfKyYzmmgLqjt4aXfdvvXB5UhVL66yAmw5W0OeP7pX6PnXVas/9ydpgNqdK3BOuhZDUsYpXEXH0+ZDH7/5zW+YN28eCxcupHfv3rz++usnPki0qU9/POBP0gDLtxRwqCC4yanoePTJmQGNZVW3g+oNXwftp4mKRReXGrRd0elRjFHU7l0X8vyKIXQNGbVBtT2/UNs6qTa/o/7qq6/Q6/W4XC6Kioro169f0D5WqxWrNbDSVmFhYdB+onXY7MHfIHUhtomOp3rjYrwNu7a4g4v6o9WRdtVvMGX0RJ+chav0WG0YRUPSrHsofO/ZgNKoxxnSe2HqOiDk51rOmuGrH3JsbDqqx1AMqV1P+3o6ijZP1Hq9nj179nDLLbeg0+l48MHgcagFCxYwf750Ig6X6Wd3Y/mWfLzHVqV172IJmm0iOiavy9ns+6buQ0iZeTf6eN/ddMbcZ6nZ+j1eex3mwRMpW/rfoCStibaQMPFqYodORlFC/xIf3WMYmbc+T+3u1ejiUjEPkcYBDSmqqqon3u3kLVq0iOeeC5zm07NnT958803/63fffZdPP/2Ud98N7I/W1B31nDlzWLJkCVlZUosilKLyOr5elYPb42X62G5kp8We8rl255Tz46Y8Ei0mZozrLlMAOwln8RHy33y0iXKjCtl3z0efkA6A116LdeM3uKtKiRkwDkNqNw6/eCuN152bh55H6ux7g87mrqnEWXgQY0ZvtNHBTQVEvVa7o54xYwYzZswI2OZwOPjuu++YOnUqABdffDHPP/980LEWiwWLRf7jTkZVjYNfvfwjVTW+O6JvVufw8oPn0SU55pTO1797Iv3lLrrT0VqSMHUbjD1nG6gqakA/QxVPnRVdbBK2Q1spW/IfXGW+YQ/rxsWkXHK/ryVXg3s/xRhF4nk3BH1Ozc4VFH/+F/C4UXQGUi//FTF9ZKppU9p06EOn0/Hkk0+Snp7O4MGDWbRoESNHtqwKl2jeqm1H/UkawObw8MPGPK6bHvwMQIimlH37Jrb9G5p8v/Sbf+Gxlvm7itdTqd6yFJ0lCXdVybFtCqmz70cXouBS2XcL/A8LVbeTsm/+JYm6GW2aqLVaLS+++CKPP/44Ho+HtLQ0nnnmmbYMocOKNgX/V4baJkRz7Ie3N/u+v79hCK7SfDw19Qtc9GndiOk3Jmg/VVWDEr27qgS3tQydJenkAu4k2vw7efTo0Xz88cdt/bEd3rghXejbNZ69RyoByEwxM+UseWouTo6xSy/cVcUnfZyiN+Bp0AUGwFWUg+r1BC0zVxQFQ2pXnIWHArbX7llN3FkzTz7oTkBuuToIvU7LvHsnsmlvCW6Pl1H9U9Hrmi83KURjSdNuwVNbiT1310kdlzL7Pqo3LsaWs82/zZDes8mSp7HDp1D29b8CtmmjZeFbU6TWRwei1WoYPSCNswd3kSQtTonOkkTGTU+Tfd+r6JMy/dsN6T3xdRI4vmP9LKDo3qOI6X82yRf9HGNWf//+qRff1+TnxA45D2NGH/9rU9dBxPQfe+YupINptel5Z1peXh5TpkyR6XlCtBHV68F+eAeKMRpTRm+cJbnY83ZjyuyLPjkL+5GdKHpjUNH/UMMdIc+verHn7kJRtBiz+kk9mWbI0EcH5vWqLFx+kDXbC8lIieG66f1IigvuBC1EKIpGS1SPof7XhpRsDCnZ/tdN1ZMOlaS9DhvVW5bgrqnAPHACxvQeKIqGqK6DznzgHZAk6g7skx/28+aXOwHYdqCUfUcqeflXk8MblOh0VNXL0befwHFsxkjVmoVk3PAkpuzQy8lFMBmj7sCWby0IeH2woIqCkpom9haidTjy9vqTNABeD9YN34QvoHZIEnUHlpYQHfDaoNcSH2sMUzSis1J0weUHQm0TTZNE3YHdMKM/qQm+MWm9TsPtFw8i2qQPc1SiPXMU5WDduBhnyZEWH2Ps0pPoPvX1zDXGaOLGzmqN8DosGaPuwLJSY/nnb6ZyqMBKSkIUcWa5mxanrmr915R989qxVwrJM+/CMnxKs8d43U40OgNpVz2M7cAm3DUVxPQ5C22MzJk+GZKoOzitVkPv7PhwhyE6gIqf3mvwSqXip/ebTNT2vD2UfDEfV1kBpq4DSb3kl0T3HtU2gXZAMvQhhGgRrzOw9KnXYQt47baWYs/djdftpPjTl3CV+R5m24/spHSxdHI6HXJHLYRokrMsn6pVn+Kx1aAxRuFt0DtRY6gfSqtY8bGv0a3qRWtODCjOBM0XcxInJolaCBGS12nj6FuP4akN3S/TU12O6nHhsdVSsexdfxstT005KBr/awBT98FtEnNHJUMfQoiQbIe2NpmkAQxpPVC0el/JUq8n8M0GSVoXn0rS1FtaKcrOQRK1ECIkT6MxaACtJRk4VnTpkvt9f0/thr7B0vIgigZtlLlVYuwsZOhDCBFS3d61QdvSrnoEY2q3gHoeiqKQctHPKfrkxfqGAJ76rvWNizaJkyd31EKIkEI1uNVo9SGLLpV+/S881lJfgva4UYy+VbFaSzKKVoez+HCrx9uRSaIWQoRkGXkBDWtQm7oOCqied5zHVo2zKLBbi6IzoDHF4LGWUr1lKflv/gZXRWFrh9xhydCHECKkmH5j6HLjk9TuXo0+Po3Y4VND7qcxxaCLT8VdWd/CSxsVi6s01/9adTmo2bGchAlXtnrcHZHcUQshmhTVdRDJ028jbswsNAZTyH0URUPqxb9AF58KgLFLb2JHTAvaT2OSB4qnSu6ohRAt4rXXUr19GarLgXnQxICO4abs/mTf/TdURx0aUwyq20XtrhU48vYAvpkhsUPODVfo7Z4kaiFEk7wuB9VbvsdVcZS63atxW0sBqFz1CWlXPIwtZxsavZHYYeejjYlDMcUAoOj0ZNz0NPac7aheD1E9hraoPZcITRK1EKJJhe8+jf3IzqDtXlsNR//3/8Drm4Zn3biYrDv+jMZQ3+pNUTQBrbzEqZMxaiFESI7CQyGTtJ+3fq60u6qYur3r2yCqzkkStRAiJEXXdJMJRR9c21y6trQeSdRCiJAMyVnEDBxfv0FnJH7ClSRNv42Muc+hjU2s3ze9F9F9RoYhys5BxqiFEE1KvfSX2Iaeh7uqhOg+o9E1SM7Zd7xE7Z41KAYTMX3OQtFKm7fWIolaCNEkRdEQ3WtEyPc0phhih53fxhF1TjL0IYQQEU4StRDihFTVi+3ITuwF+8MdSqckQx9CiCCq1+NfoOJ11FHw1uP+wkvRvUeRdtUjZ3QBS8PPE8EkUQsh/FzlRyn+/C848vdi7NKLlIvvx3Zwc0B1vLr9G6g7sImYPqNP+/O8bielX/2Dmh3L0cbEkTTtFswDzjnt83Y0MvQhhPAr+fLvOPL3AuA4eoCShfNx11QE7eepqTwjn1e1ZiE1234ErwdPdTnFn73cbPuvzkoStRDCz1Gwr9Hr/cQMnACa+l++NaYYYvqedWY+Lz/w8/C4cTSqbS1k6EMI0YCp60BsBzc3eD0AU3oPMm58EuvGxSg6A3FjZqGNiQs61lGUQ/Wmb1F0BiyjL0Qfn3biz+s2kLp96/yvFb0RY5feZ+RaOhJJ1EIIv5SZd1Py1d+x5+7ClNmX5IvuAsCU1R9TVv8mj3OW5lHw5m9Q3U4Aqrf9QNbtL1C3bz3Osnxieo8KWaAp7qyZuKtKqNm+DK05gaQpc6URbgiKqqpquINoiby8PKZMmcKSJUvIysoKdzhCiAbKf/gflSs+CthmSO+Js/Cg/3XKxfcRO2RyG0fWMcgYtRDitIXq3tIwSQNY13/dVuF0OJKohRCnLXbY+eiT63/TNWb2AyUwvYSquCdaJmxj1Dt37uTqq69m+/bt4QpBCHGGaKPMZN3+J2wHt6Lo9Ji6D6Z8yVtUrfn82A46EsZfEd4g27GwJGqbzcZTTz2Fy+UKx8cLIZqgqqrvAWDJEaJ7DMOY0Rt7/l5sh7ZiTO9JdO/QpUw9tmpqdiwHVcU8aAKKoiFp6lxi+o3FWZpHVM+haAzRVK3/GlQv5kET0EZbcFvLqNm1Ao0hGn1yBvYjuzAkZxPd9ywURWnjq49cYUnUf/jDH5g7dy4bN24M+b7VasVqtQZsKywsbIvQhOjUyhb/G+v6rwCo+OEdYkdMpXrTt/7348ZdStL5NwYc47HVkP/6Q7irSgBfP8Ws219AG23BlN0fU3Z/PPZa8v/1a9xVxb59Vn5C2pUPUfju03jttUFxxI6cTsqMO1vrMtudNk/US5YswW63c+GFFza5z4IFC5g/f34bRiWE8DrqsG5c3GCLSvXWHwL2sa77ioRJ16Bp0M2lducKf5IG8FSXU7N9GXFjZtXvs2ulP0kDeGrKKf/+7ZBJGqB603ckTp4jU/WOabVEvWjRIp577rmAbT179qSmpoY333yz2WPnzp3LZZddFrCtsLCQOXPmnOkwhRABImm2biTFEl6tNutjxowZLFu2LODPzJkzqaysZM6cOVxyySUAXHLJJdTU1AQca7FYyMrKCviTnp7eWqEKIQCNMRrLyAsabFGIHTo5YB/LWRcF3E0DxAwcjy4uxf9aG5uIefCkwH0GnIMuLrV+H3MiiZPnoDHFhIwldsRUtFGxp3YhHVBYF7z069ePPXv2tGhfWfAiROtr/mFiD6J7jwp5XODDxIloo4OTrMdWc2wfeZh4siRRCyFEhAvrgpeWJmkhhOjMZGWiEEJEOEnUQggR4SRRCyFEhJNELYQQEU4StRBCRDhJ1EIIEeEkUQshRISTRC2EEBGu3TS39Xg8gJQ7FUJ0XOnp6eh0wWm53STqkhJfGUWpoCeE6KiaKpHRbrqQ2+12tm/fTkpKClqtNtzhtBvHy8O+/fbbUoFQtDr5ejs97f6O2mQyMXr06HCH0W6lp6dLMSvRZuTr7cySh4lCCBHhJFELIUSEk0QthBARThJ1B2exWLj33nuxWCzhDkV0AvL11jrazawPIYTorOSOWgghIpwkaiGEiHCSqDu4mpoaZs2aRV5eXrhDEZ3AwoULueiii5g+fTpvv/12uMPpMCRRd2BbtmzhuuuuIycnJ9yhiE6gqKiIF198kf/97398+umnvPfee+zfvz/cYXUIkqg7sPfff58nnniC1NTUcIciOoGVK1dy9tlnEx8fT3R0NBdccAFff/11uMPqENrNEnJx8p555plwhyA6keLiYlJSUvyvU1NT2bp1axgj6jjkjloIcUZ4vV4URfG/VlU14LU4dZKohRBnRHp6ur8cMfhKE8uw25khiVoIcUacc845rFq1ivLycmw2G4sXL2bSpEnhDqtDkDFqIcQZkZaWxgMPPMBNN92Ey+XiyiuvZOjQoeEOq0OQJeRCCBHhZOhDCCEinCRqIYSIcJKohRAiwkmiFkKICCeJWgghIpwkaiGEiHAyj1qcsn79+tG3b180msCf93/961/Jyspqkxjy8vKYNm0affv2Ddh+/vnn84tf/KJVPvPOO+/kggsu4PLLL2+V8x934403MmfOHC688MIm93nllVeoqKjg8ccfb/F58/LymD17Nps2bToTYYo2IIlanJYFCxaQmJgY1hhMJhOfffZZWGMQojVJohat5sMPP+SNN95Ao9GQkJDA888/T5cuXXjvvfd466230Gg0JCcn89hjj9GjRw8effRRzGYze/bsobCwkH79+vH8888TExNzSp+fl5fHnDlz6NWrF/n5+bz11lvk5eXxpz/9CZvNhkaj4d577+W8884D4IMPPuCdd97B6/USHx/PY489Rq9evSgqKuLRRx+luLiYjIwMysrK/J+xfv165s2bh81mQ6/X88tf/pJJkybx8ccfs3jxYrxeLwUFBaSlpXH11Vfz3//+l5ycHG655RZuvfXWFl/LP/7xD5YsWYLdbsdms/HII48wbdo0AA4cOMCcOXOoqqpiwIABPPHEE5jNZoqKivh//+//cfToUVwuFzNnzuTnP//5Kf1bijBThThFffv2VWfNmqVefPHF/j933323qqqqumvXLnXs2LFqQUGBqqqq+sYbb6iPPfaYunLlSnXq1KlqWVmZqqqq+tFHH6kzZsxQvV6v+sgjj6jXXHON6nA4VKfTqV566aXqhx9+2GwMubm5av/+/QNiuOyyy/zv9e3bV123bp2qqqpaWVmpTp8+Xc3NzVVVVVULCwvVSZMmqfn5+eqaNWvU66+/Xq2rq1NVVVV/+ukn9cILL1RVVVXvvvtu9cUXX1RVVVVzcnLU4cOHqx999JFaXl6ujhs3Tt28ebOqqqq6d+9edcyYMeqRI0fUjz76SB01apRaUFCgejwe9aKLLlLvu+8+1ePxqLt27VKHDBmiejyeZq/thhtuUBctWqTm5eWpN954o2qz2VRVVdUvvvhCnTVrlqqqqvqXv/xFnTx5slpWVqZ6vV71V7/6lTpv3jxVVVX1xhtvVJcsWaKqqqra7Xb1xhtvVL/88ks1NzdXHT58+An/f0XkkDtqcVqaGvpYtWoVEyZMoEuXLgDcfPPNAMybN4+LLrrIf8zll1/OM888428VNnHiRAwGAwB9+/alqqrqhDE0N/Sh0+kYPnw4AJs3b6akpIR77rnH/76iKOzZs4d169Zx+PBhrr32Wv97VquVyspKVq5cySOPPAJAt27dGDt2LABbt26la9euDBs2DIA+ffowcuRI1q5di6IoDBkyxH/9WVlZTJgwAY1GQ3Z2Ng6HA5vN1qLfFjIzM5k3bx4LFy7k8OHDbNmyhdraWv/706ZN8/97XnHFFcybN4+6ujrWrVtHVVUVL7/8MgB1dXXs3r1b6m+0Q5KoRavQarUBtYjtdjv5+fl4vd6gfVVVxe12A76ke5yiKKinWYrGYDCg0/m+zD0eD7169eKDDz7wv19UVERiYiJr1qzhkksu4aGHHgJ8tZWLi4uJi4sLiqPh+RrXWz5+LXq93v8Dp/FxJ2vHjh3cfffd3HzzzYwfP56zzjqLJ5980v++Vqv1/93r9aLT6fB6vaiqyrvvvktUVBQA5eXlGI1GKioqTikOET4yPU+0irFjx7Jq1SqKi4sBePfdd/njH//IxIkT+eqrrygvLwfgo48+Ij4+nm7durV6TMOHD+fw4cOsW7cOgF27dnHBBRdQVFTEhAkT+PLLL/3xvvPOO8ydOxfw3eW/9957ABQUFLBmzRr/+Q4ePOjvYrJv3z7WrVvHmDFjzmjc69atY/Dgwdxyyy2MGTOGJUuW4PF4/O8vXbqUqqoqPB4P77//PpMmTcJsNjN8+HDeeOMNwPfbwXXXXceSJUvOaGyibcgdtTgtc+fODZqe9+CDD3Luuefy0EMPcfvttwOQkpLCs88+S1paGjfffDNz587F6/WSmJjIq6++GnSO1pCYmMhf/vIX5s2bh8PhQFVV5s2bR1ZWFllZWfzsZz/j1ltvRVEUzGYz8+fPR1EUnnjiCX7zm98wY8YM0tPT6d+/v/98L7/8Mk899RR2ux1FUXjuuefo0aPHGZ36NmvWLBYvXsyMGTPwer2cd955VFVVUVNTA0CvXr248847sVqtjBo1ijvuuAOAP/3pTzz11FPMnj0bp9PJrFmzuPjii6UjfTskZU6FECLCyR21iHjXX399wMOzht5++23MZnMbR3RmrF69mueeey7ke2PHjuW3v/1tG0ckIpXcUQshRISTh4lCCBHhJFELIUSEk0QthBARThK1EEJEOEnUQggR4f4/y1EvSH5kp0AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "training_set = pd.concat([X_train, y_train], axis = 1)\n",
    "sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "\n",
    "sns.catplot(x=\"Econ_Freedom_label\", y=\"4_trade\", data=training_set);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is clearly a categorical variable so it is either going to be I would consider this to binary, the distirbution based upon jus the training set it would appear that this is a binomial distribution just becasue of the fact that it is predicting either 0 or a 1. Later on if there were more variables to identify this would notion would change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I ran into some errors which wouldn't let me use f1 as a scorer, becasue it wasnt able to attribute the positive label aka yes or no of the prediction, So I modiified the F1 scorer to make it so that 1 is yes and 0 is no\n",
    "###### 1 = greater than 5, 0 is less than or equal to 5. So in other words is the value above 5 Yes everythiing else no. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN Optimized\n",
      "\n",
      "\n",
      "Train Accuracy:  0.9771024146544546\n",
      "\n",
      "Test Accuracy:  0.9667221297836939\n",
      "\n",
      "Test - No. Of Correct Predictions 581.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 49  15]\n",
      " [  5 532]]\n",
      "\n",
      "Test Precision = 0.965638\n",
      "Test Recall = 0.966722\n",
      "Test F1 Score = 0.965466\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.77      0.83        64\n",
      "           1       0.97      0.99      0.98       537\n",
      "\n",
      "    accuracy                           0.97       601\n",
      "   macro avg       0.94      0.88      0.91       601\n",
      "weighted avg       0.97      0.97      0.97       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "knn_base = KNeighborsClassifier()\n",
    "knn_base.fit(X_train , y_train)\n",
    "prediction = knn_base.predict(X_test)\n",
    "training_predictioin = knn_base.predict(X_train)\n",
    "\n",
    "print('\\nKNN Optimized\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamater Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 128 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=4)]: Done 728 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=4)]: Done 1200 out of 1200 | elapsed:   12.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=KNeighborsClassifier(), n_jobs=4,\n",
       "             param_grid={'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
       "                         'metric': ['manhattan', 'minkowski', 'euclidean'],\n",
       "                         'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
       "                         'weights': ['uniform', 'distance']},\n",
       "             scoring='accuracy', verbose=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "f1_scorer = make_scorer(f1_score, pos_label= '1')\n",
    "param_grid = {\n",
    "    'metric':['manhattan', 'minkowski', 'euclidean'],\n",
    "    'n_neighbors':[1,2,3,4,5,6,7,8,9,10],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    \n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_knn = GridSearchCV(knn, param_grid, scoring = 'accuracy', cv = 5, n_jobs = 4, verbose = True)\n",
    "grid_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CV = ``` StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.972557\n",
      "Optimal Hyperparameter Values:  {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 8, 'weights': 'distance'}\n",
      "Worst Hyperparameter Valeus:  {'algorithm': 'auto', 'metric': 'minkowski', 'n_neighbors': 1, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "knn_best_params = grid_knn.best_params_\n",
    "knn_results = pd.DataFrame(grid_knn.cv_results_)\n",
    "worst_score = knn_results.loc[np.argmax(knn_results.rank_test_score)]\n",
    "worst_score_params = worst_score.params\n",
    "print(\"Best Score (accuracy): %f\" % grid_knn.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", knn_best_params)\n",
    "print(\"Worst Hyperparameter Valeus: \", worst_score_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using CV = ```5```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.970023\n",
      "Optimal Hyperparameter Values:  {'algorithm': 'auto', 'metric': 'manhattan', 'n_neighbors': 10, 'weights': 'distance'}\n",
      "Worst Hyperparameter Valeus:  {'algorithm': 'auto', 'metric': 'minkowski', 'n_neighbors': 2, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "knn_best_params = grid_knn.best_params_\n",
    "knn_results = pd.DataFrame(grid_knn.cv_results_)\n",
    "worst_score = knn_results.loc[np.argmax(knn_results.rank_test_score)]\n",
    "worst_score_params = worst_score.params\n",
    "print(\"Best Score (accuracy): %f\" % grid_knn.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", knn_best_params)\n",
    "print(\"Worst Hyperparameter Valeus: \", worst_score_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes \n",
    "- It would seem that having a higher number of N_neighbors is a requriement, this could mean that the data is precise enough in order to make a good estimate, since with lower neighbors would mean that the data has either enough data to account for small chages or that the data is to spread out resulting in small differences from being notices. \n",
    "- Also there seems to be a discrpency when you change the type of folds being used. when using the folds where you use stratified CV, it produces the number of neighbors being lower at 8, although it produces a worse accuracy with 585/601 isntead of with just cv = 5 it makes 587 /601. \n",
    "    - I think that this is because the stratified make sure that each fold has enough of each value in order to decide and model the dataset. So compared to just splitting it into 5 chunks the prediction is going to be more realistic as well as better balanced which is why 8 was shown to be a better predictor. But becasue eerything is more balanced, the actual prediction is going to lose more since straight up 5 balance could've been able to show more things. \n",
    "    - To restate what was just said, equal balancing of the dataset allows for better ability for KNN to use less value for K becasue the dataset's model fit is closer to each other and is able to capture smaller differences, But the actual prediction is going to be lesser becasue it doesn't have the ability or the notion that it can predict outcomes that are not balanced which the test dataset was kind of made to represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN Optimized w/ CV = CV \n",
      "\n",
      "\n",
      "Train Accuracy:  1.0\n",
      "\n",
      "Test Accuracy:  0.9733777038269551\n",
      "\n",
      "Test - No. Of Correct Predictions 585.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 53  11]\n",
      " [  5 532]]\n",
      "\n",
      "Test Precision = 0.972719\n",
      "Test Recall = 0.973378\n",
      "Test F1 Score = 0.972797\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.83      0.87        64\n",
      "           1       0.98      0.99      0.99       537\n",
      "\n",
      "    accuracy                           0.97       601\n",
      "   macro avg       0.95      0.91      0.93       601\n",
      "weighted avg       0.97      0.97      0.97       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn2 = KNeighborsClassifier(metric = 'manhattan', n_neighbors = 8, weights = 'distance')\n",
    "knn2.fit(X_train, y_train)\n",
    "prediction = knn2.predict(X_test)\n",
    "training_predictioin = knn2.predict(X_train)\n",
    "\n",
    "print('\\nKNN Optimized w/ CV = CV \\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN Optimized w/ CV = 5\n",
      "\n",
      "\n",
      "Train Accuracy:  1.0\n",
      "\n",
      "Test Accuracy:  0.9767054908485857\n",
      "\n",
      "Test - No. Of Correct Predictions 587.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 52  12]\n",
      " [  2 535]]\n",
      "\n",
      "Test Precision = 0.976454\n",
      "Test Recall = 0.976705\n",
      "Test F1 Score = 0.975826\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.81      0.88        64\n",
      "           1       0.98      1.00      0.99       537\n",
      "\n",
      "    accuracy                           0.98       601\n",
      "   macro avg       0.97      0.90      0.93       601\n",
      "weighted avg       0.98      0.98      0.98       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn3 = KNeighborsClassifier(metric = 'manhattan', n_neighbors = 10, weights = 'distance')\n",
    "knn3.fit(X_train, y_train)\n",
    "prediction = knn3.predict(X_test)\n",
    "training_predictioin = knn3.predict(X_train)\n",
    "\n",
    "print('\\nKNN Optimized w/ CV = 5\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2 set score: 0.98\n",
      "Test R^2 set score: 0.97\n",
      "\n",
      "Logistic Regression Base\n",
      "\n",
      "R squared score: 0.6502560521415269 \n",
      "Root Mean Squared Error: 0.033277870216306155 \n",
      "Mean Absolute Error 0.033277870216306155\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "logreg_base = LogisticRegression()\n",
    "logreg_base.fit(X_train, y_train)\n",
    "prediction = logreg_base.predict(X_test)\n",
    "print(\"Training R^2 set score: {:.2f}\".format(logreg_base.score(X_train, y_train)))\n",
    "print(\"Test R^2 set score: {:.2f}\".format(logreg_base.score(X_test, y_test)))\n",
    "\n",
    "r2 = r2_score(y_test , prediction)\n",
    "RMSE = mean_squared_error(y_test, prediction)\n",
    "MAE = mean_absolute_error(y_test, prediction)\n",
    "print(\"\\nLogistic Regression Base\")\n",
    "print('\\nR squared score:', r2, '\\nRoot Mean Squared Error:', RMSE, '\\nMean Absolute Error', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamater Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1600 candidates, totalling 8000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  68 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=4)]: Done 2556 tasks      | elapsed:   18.9s\n",
      "[Parallel(n_jobs=4)]: Done 5140 tasks      | elapsed:   49.8s\n",
      "[Parallel(n_jobs=4)]: Done 7940 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done 8000 out of 8000 | elapsed:  1.6min finished\n",
      "C:\\Users\\scott\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1320: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "C:\\Users\\scott\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=4,\n",
       "             param_grid={'C': array([1.00000000e-04, 2.63665090e-04, 6.95192796e-04, 1.83298071e-03,\n",
       "       4.83293024e-03, 1.27427499e-02, 3.35981829e-02, 8.85866790e-02,\n",
       "       2.33572147e-01, 6.15848211e-01, 1.62377674e+00, 4.28133240e+00,\n",
       "       1.12883789e+01, 2.97635144e+01, 7.84759970e+01, 2.06913808e+02,\n",
       "       5.45559478e+02, 1.43844989e+03, 3.79269019e+03, 1.00000000e+04]),\n",
       "                         'max_iter': [100, 1000, 2500, 5000],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
       "                         'solver': ['lbfgs', 'newton-cg', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring='neg_root_mean_squared_error', verbose=True)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "param_grid_log = {\n",
    "        'penalty':['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'C': np.logspace(-4,4,20),\n",
    "        'solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga'],\n",
    "        'max_iter': [100, 1000, 2500, 5000]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "log_grid = GridSearchCV(logreg, param_grid = param_grid_log, scoring = 'neg_root_mean_squared_error', verbose = True, cv=5, n_jobs=4)\n",
    "log_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing out the Best + Worst Performing Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (neg_root_mean_squared_error): -0.163087\n",
      "Optimal Hyperparameter Values:  {'C': 0.23357214690901212, 'max_iter': 1000, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Worst Hyperparameter Valeus:  {'C': 0.0001, 'max_iter': 100, 'penalty': 'l1', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "log_best_params = log_grid.best_params_\n",
    "log_results = pd.DataFrame(log_grid.cv_results_)\n",
    "worst_score = log_results.loc[np.argmax(log_results.rank_test_score)]\n",
    "worst_score_params = worst_score.params\n",
    "print(\"Best Score (neg_root_mean_squared_error): %f\" % log_grid.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", log_best_params)\n",
    "print(\"Worst Hyperparameter Valeus: \", worst_score_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the hyperparamater tuning on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2 set score: 0.98\n",
      "Test R^2 set score: 0.98\n",
      "\n",
      "Logistic Regression Optimized\n",
      "\n",
      "R squared score: 0.7872189768100548 \n",
      "Root Mean Squared Error: 0.019966722129783693 \n",
      "Mean Absolute Error 0.019966722129783693\n"
     ]
    }
   ],
   "source": [
    "logreg2 = LogisticRegression(C = 0.23357214690901212, max_iter = 1000, penalty = 'l2', solver = 'saga')\n",
    "logreg2.fit(X_train , y_train)\n",
    "prediction = logreg2.predict(X_test)\n",
    "print(\"Training R^2 set score: {:.2f}\".format(logreg2.score(X_train, y_train)))\n",
    "print(\"Test R^2 set score: {:.2f}\".format(logreg2.score(X_test, y_test)))\n",
    "\n",
    "r2 = r2_score(y_test , prediction)\n",
    "RMSE = mean_squared_error(y_test, prediction)\n",
    "MAE = mean_absolute_error(y_test, prediction)\n",
    "\n",
    "print(\"\\nLogistic Regression Optimized\")\n",
    "print('\\nR squared score:', r2, '\\nRoot Mean Squared Error:', RMSE, '\\nMean Absolute Error', MAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Base\n",
      "\n",
      "\n",
      "Train Accuracy:  0.9912572855953372\n",
      "\n",
      "Test Accuracy:  0.978369384359401\n",
      "\n",
      "Test - No. Of Correct Predictions 588.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 53  11]\n",
      " [  2 535]]\n",
      "\n",
      "Test Precision = 0.978127\n",
      "Test Recall = 0.978369\n",
      "Test F1 Score = 0.977641\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89        64\n",
      "           1       0.98      1.00      0.99       537\n",
      "\n",
      "    accuracy                           0.98       601\n",
      "   macro avg       0.97      0.91      0.94       601\n",
      "weighted avg       0.98      0.98      0.98       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "svm_base = SVC().fit(X_train, y_train)\n",
    "\n",
    "\n",
    "prediction = svm_base.predict(X_test)\n",
    "training_predictioin = svm_base.predict(X_train)\n",
    "\n",
    "print('\\nSVM Base\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparamater Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=5, random_state=42, test_size=0.2,\n",
       "            train_size=None),\n",
       "             estimator=SVC(), n_jobs=4,\n",
       "             param_grid={'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
       "                         'gamma': [0.5, 1, 5, 10, 50, 100, 500, 1000],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmmoid']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "    'kernel':['linear', 'poly', 'rbf', 'sigmmoid'],\n",
    "    'gamma':[0.5, 1, 5, 10 ,50, 100, 500, 1000]\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "grid_svm = GridSearchCV(svm, param_grid , scoring = 'accuracy', cv = cv, n_jobs = 4)\n",
    "grid_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.988358\n",
      "Optimal Hyperparameter Values:  {'C': 0.8, 'gamma': 0.5, 'kernel': 'linear'}\n",
      "Worst Hyperparameter Valeus:  {'C': 0.9, 'gamma': 1000, 'kernel': 'sigmmoid'}\n"
     ]
    }
   ],
   "source": [
    "svm_best_params = grid_svm.best_params_\n",
    "svm_results = pd.DataFrame(grid_svm.cv_results_)\n",
    "worst_score = svm_results.loc[np.argmax(svm_results.rank_test_score)]\n",
    "worst_score_params = worst_score.params\n",
    "print(\"Best Score (accuracy): %f\" % grid_svm.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", svm_best_params)\n",
    "print(\"Worst Hyperparameter Valeus: \", worst_score_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes \n",
    "Since I saw some differences between the scores using cv = the stratified split vs just 5 I test that here looking for the most accurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(), n_jobs=4,\n",
       "             param_grid={'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
       "                         'gamma': [0.5, 1, 5, 10, 50, 100, 500, 1000],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmmoid']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "    'kernel':['linear', 'poly', 'rbf', 'sigmmoid'],\n",
    "    'gamma':[0.5, 1, 5, 10 ,50, 100, 500, 1000]\n",
    "}\n",
    "\n",
    "svm = SVC()\n",
    "grid_svm = GridSearchCV(svm, param_grid , scoring = 'accuracy', cv = 5, n_jobs = 4)\n",
    "grid_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.990424\n",
      "Optimal Hyperparameter Values:  {'C': 0.7, 'gamma': 0.5, 'kernel': 'linear'}\n",
      "Worst Hyperparameter Valeus:  {'C': 0.9, 'gamma': 1000, 'kernel': 'sigmmoid'}\n"
     ]
    }
   ],
   "source": [
    "svm_best_params = grid_svm.best_params_\n",
    "svm_results = pd.DataFrame(grid_svm.cv_results_)\n",
    "worst_score = svm_results.loc[np.argmax(svm_results.rank_test_score)]\n",
    "worst_score_params = worst_score.params\n",
    "print(\"Best Score (accuracy): %f\" % grid_svm.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", svm_best_params)\n",
    "print(\"Worst Hyperparameter Valeus: \", worst_score_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Optimized w/ CV = CV\n",
      "\n",
      "\n",
      "Train Accuracy:  0.9933388842631141\n",
      "\n",
      "Test Accuracy:  0.9900166389351082\n",
      "\n",
      "Test - No. Of Correct Predictions 595.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 59   5]\n",
      " [  1 536]]\n",
      "\n",
      "Test Precision = 0.989967\n",
      "Test Recall = 0.990017\n",
      "Test F1 Score = 0.989874\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        64\n",
      "           1       0.99      1.00      0.99       537\n",
      "\n",
      "    accuracy                           0.99       601\n",
      "   macro avg       0.99      0.96      0.97       601\n",
      "weighted avg       0.99      0.99      0.99       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm2 = SVC(C = 0.8, gamma = 0.5, kernel = 'linear')\n",
    "svm2.fit(X_train, y_train)\n",
    "prediction = svm2.predict(X_test)\n",
    "training_predictioin = svm2.predict(X_train)\n",
    "\n",
    "print('\\nSVM Optimized w/ CV = CV\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Optimized w/ CV = CV\n",
      "\n",
      "\n",
      "Train Accuracy:  0.9933388842631141\n",
      "\n",
      "Test Accuracy:  0.9900166389351082\n",
      "\n",
      "Test - No. Of Correct Predictions 595.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 59   5]\n",
      " [  1 536]]\n",
      "\n",
      "Test Precision = 0.989967\n",
      "Test Recall = 0.990017\n",
      "Test F1 Score = 0.989874\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        64\n",
      "           1       0.99      1.00      0.99       537\n",
      "\n",
      "    accuracy                           0.99       601\n",
      "   macro avg       0.99      0.96      0.97       601\n",
      "weighted avg       0.99      0.99      0.99       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm2 = SVC(C = 0.7, gamma = 0.5, kernel = 'linear')\n",
    "svm2.fit(X_train, y_train)\n",
    "prediction = svm2.predict(X_test)\n",
    "training_predictioin = svm2.predict(X_train)\n",
    "\n",
    "print('\\nSVM Optimized w/ CV = CV\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes \n",
    "As it appears there isn't as much of a descrpency between using CV = the stratified shulffle and cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Optimized w/ CV = CV\n",
      "\n",
      "\n",
      "Train Accuracy:  0.9937552039966694\n",
      "\n",
      "Test Accuracy:  0.9900166389351082\n",
      "\n",
      "Test - No. Of Correct Predictions 595.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 59   5]\n",
      " [  1 536]]\n",
      "\n",
      "Test Precision = 0.989967\n",
      "Test Recall = 0.990017\n",
      "Test F1 Score = 0.989874\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        64\n",
      "           1       0.99      1.00      0.99       537\n",
      "\n",
      "    accuracy                           0.99       601\n",
      "   macro avg       0.99      0.96      0.97       601\n",
      "weighted avg       0.99      0.99      0.99       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm2 = SVC(C = 0.7, gamma = 0.5, kernel = 'linear')\n",
    "svm2.fit(X_train, y_train)\n",
    "prediction = svm2.predict(X_test)\n",
    "training_predictioin = svm2.predict(X_train)\n",
    "\n",
    "print('\\nSVM Optimized w/ CV = CV\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Want to look at potentially some changes using SVM. Maybe expand the gamma and c to be 0 - 1 with 0.01 intervals. and look and see the changes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking at the grid search with cv = cv stratified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedShuffleSplit(n_splits=5, random_state=42, test_size=0.2,\n",
       "            train_size=None),\n",
       "             estimator=SVC(gamma=0.5, kernel='linear'), n_jobs=4,\n",
       "             param_grid={'C': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_range = np.arange(0, 1, 0.01)\n",
    "param_grid = dict(C=C_range)\n",
    "svm_test = SVC(kernel = 'linear', gamma = 0.5)\n",
    "grid_test_svm = GridSearchCV(svm_test, param_grid, scoring = 'accuracy', cv = cv, n_jobs = 4)\n",
    "grid_test_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.989189\n",
      "Optimal Hyperparameter Values:  {'C': 0.9400000000000001}\n",
      "Worst Hyperparameter Valeus:  {'C': 0.0}\n"
     ]
    }
   ],
   "source": [
    "svm_best_params = grid_test_svm.best_params_\n",
    "svm_results = pd.DataFrame(grid_test_svm.cv_results_)\n",
    "worst_score = svm_results.loc[np.argmax(svm_results.rank_test_score)]\n",
    "worst_score_params = worst_score.params\n",
    "print(\"Best Score (accuracy): %f\" % grid_test_svm.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", svm_best_params)\n",
    "print(\"Worst Hyperparameter Valeus: \", worst_score_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking at gridsearch with cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(gamma=0.5, kernel='linear'), n_jobs=4,\n",
       "             param_grid={'C': array([0.  , 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1 ,\n",
       "       0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2 , 0.21,\n",
       "       0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3 , 0.31, 0.32,\n",
       "       0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4 , 0.41, 0.42, 0.43,\n",
       "       0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5 , 0.51, 0.52, 0.53, 0.54,\n",
       "       0.55, 0.56, 0.57, 0.58, 0.59, 0.6 , 0.61, 0.62, 0.63, 0.64, 0.65,\n",
       "       0.66, 0.67, 0.68, 0.69, 0.7 , 0.71, 0.72, 0.73, 0.74, 0.75, 0.76,\n",
       "       0.77, 0.78, 0.79, 0.8 , 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87,\n",
       "       0.88, 0.89, 0.9 , 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98,\n",
       "       0.99])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_range = np.arange(0, 1, 0.01)\n",
    "param_grid = dict(C=C_range)\n",
    "svm_test = SVC(kernel = 'linear', gamma = 0.5)\n",
    "grid_test_svm = GridSearchCV(svm_test, param_grid, scoring = 'accuracy', cv = 5, n_jobs = 4)\n",
    "grid_test_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score (accuracy): 0.990840\n",
      "Optimal Hyperparameter Values:  {'C': 0.68}\n",
      "Worst Hyperparameter Valeus:  {'C': 0.0}\n"
     ]
    }
   ],
   "source": [
    "svm_best_params = grid_test_svm.best_params_\n",
    "svm_results = pd.DataFrame(grid_test_svm.cv_results_)\n",
    "worst_score = svm_results.loc[np.argmax(svm_results.rank_test_score)]\n",
    "worst_score_params = worst_score.params\n",
    "print(\"Best Score (accuracy): %f\" % grid_test_svm.best_score_)\n",
    "print(\"Optimal Hyperparameter Values: \", svm_best_params)\n",
    "print(\"Worst Hyperparameter Valeus: \", worst_score_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Optimized w/ CV = CV\n",
      "\n",
      "\n",
      "Train Accuracy:  0.9937552039966694\n",
      "\n",
      "Test Accuracy:  0.9916805324459235\n",
      "\n",
      "Test - No. Of Correct Predictions 596.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 60   4]\n",
      " [  1 536]]\n",
      "\n",
      "Test Precision = 0.991636\n",
      "Test Recall = 0.991681\n",
      "Test F1 Score = 0.991592\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96        64\n",
      "           1       0.99      1.00      1.00       537\n",
      "\n",
      "    accuracy                           0.99       601\n",
      "   macro avg       0.99      0.97      0.98       601\n",
      "weighted avg       0.99      0.99      0.99       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm3 = SVC(C = 0.9400000000000001, gamma = 0.5, kernel = 'linear')\n",
    "svm3.fit(X_train, y_train)\n",
    "prediction = svm3.predict(X_test)\n",
    "training_predictioin = svm3.predict(X_train)\n",
    "\n",
    "print('\\nSVM Optimized w/ CV = CV\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM Optimized w/ CV = 5\n",
      "\n",
      "\n",
      "Train Accuracy:  0.9937552039966694\n",
      "\n",
      "Test Accuracy:  0.9900166389351082\n",
      "\n",
      "Test - No. Of Correct Predictions 595.0 / 601\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[ 59   5]\n",
      " [  1 536]]\n",
      "\n",
      "Test Precision = 0.989967\n",
      "Test Recall = 0.990017\n",
      "Test F1 Score = 0.989874\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.92      0.95        64\n",
      "           1       0.99      1.00      0.99       537\n",
      "\n",
      "    accuracy                           0.99       601\n",
      "   macro avg       0.99      0.96      0.97       601\n",
      "weighted avg       0.99      0.99      0.99       601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm3 = SVC(C =  0.68, gamma = 0.5, kernel = 'linear')\n",
    "svm3.fit(X_train, y_train)\n",
    "prediction = svm3.predict(X_test)\n",
    "training_predictioin = svm3.predict(X_train)\n",
    "\n",
    "print('\\nSVM Optimized w/ CV = 5\\n')\n",
    "training_accuracy = np.mean(training_predictioin == y_train)\n",
    "test_accuracy = np.mean(prediction == y_test)\n",
    "\n",
    "print(\"\\nTrain Accuracy: \", training_accuracy)\n",
    "print(\"\\nTest Accuracy: \", test_accuracy)\n",
    "\n",
    "test_size = X_test.shape[0]\n",
    "test_no_correct = test_size  * test_accuracy\n",
    "\n",
    "print (\"\\nTest - No. Of Correct Predictions\", test_no_correct, \"/\", test_size)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, prediction))\n",
    "\n",
    "\n",
    "precision_test = precision_score(y_test, prediction,average='weighted') \n",
    "print(\"\\nTest Precision = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, prediction,average='weighted')\n",
    "print(\"Test Recall = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, prediction,average='weighted')\n",
    "print(\"Test F1 Score = %f\" % f1_test)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Notes \n",
    "So it would appear specifically with this case that the stratified dataset is better sutited to produce these kinds of results compared to using it with KNN. I think this could be due to how specifically the prediction is met, SVM utilizes the Linear kernel, while for KNN its the distance between everything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No real change in F1 or even accuracy between the hyperparamter scaling. \n",
    "Default SVC \n",
    "- Gamma = \"Scale\" \n",
    "    - this is equal to 1/(number of features * X.varriance) \n",
    "- Kernal = rbf\n",
    "- C = 1.0 \n",
    "\n",
    "\n",
    "Hyperparamaters \n",
    "\n",
    "\n",
    "- Gamma = 0.4\n",
    "- Kernal = Linear\n",
    "- C = 0.4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
